[Unit]
Description=Llama-3.1-8B-Instruct Local Inference Server (Genesis Rebuild)
After=network.target

[Service]
Type=simple
User=genesis
WorkingDirectory=/home/genesis/genesis-rebuild
Environment="PATH=/home/genesis/genesis-rebuild/.venv/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin"
ExecStart=/home/genesis/genesis-rebuild/.venv/bin/python /home/genesis/genesis-rebuild/infrastructure/local_inference_server.py --model llama-3.1-8b --port 8003 --host 127.0.0.1 --n-ctx 4096 --n-threads 4

# Security directives (via Context7 MCP - llama.cpp security hardening)
PrivateTmp=true
ProtectSystem=strict
ProtectHome=true
ReadWritePaths=/home/genesis/genesis-rebuild /home/genesis/local_models
NoNewPrivileges=true
RestrictAddressFamilies=AF_INET AF_INET6
MemoryMax=8G
CPUQuota=75%

# Restart policy
Restart=on-failure
RestartSec=10
StartLimitInterval=60s
StartLimitBurst=3

# Logging
StandardOutput=journal
StandardError=journal
SyslogIdentifier=llama-3.1-8b-server

[Install]
WantedBy=multi-user.target
