#!/usr/bin/env python3
"""
DeepResearch Quality Validation Script

Validates training examples generated by DeepResearch pipeline against
quality criteria defined in DEEPRESEARCH_20K_DISTRIBUTION_PLAN.md

Features:
- Automated syntax and schema validation
- Hudson's 10-dimension quality scoring
- Batch validation of JSONL files
- Detailed error reporting and recommendations

Usage:
    python scripts/validate_deepresearch_quality.py data/deepresearch_generated/qa_agent_examples.jsonl
    python scripts/validate_deepresearch_quality.py --all  # Validate all agents
    python scripts/validate_deepresearch_quality.py --agent qa_agent --threshold 90

Requirements:
    pip install jsonschema

Author: Thon (Python Specialist)
Date: October 31, 2025
"""

import json
import sys
from pathlib import Path
from typing import Dict, List, Tuple
from dataclasses import dataclass
import statistics


# Agent-specific task categories (from templates)
AGENT_CATEGORIES = {
    "qa_agent": [
        "test_generation",
        "bug_detection",
        "code_review",
        "integration_testing",
        "performance_testing"
    ],
    "support_agent": [
        "technical_troubleshooting",
        "product_information",
        "account_management",
        "setup_onboarding",
        "escalation_handling"
    ],
    "legal_agent": [
        "contract_review",
        "regulatory_compliance",
        "terms_of_service_analysis",
        "risk_assessment",
        "legal_research"
    ],
    "analyst_agent": [
        "data_analysis",
        "market_research",
        "competitive_intelligence",
        "financial_modeling",
        "strategic_insights"
    ],
    "content_agent": [
        "blog_writing",
        "social_media_content",
        "email_campaigns",
        "technical_documentation",
        "seo_copy"
    ]
}

VALID_DIFFICULTIES = ["easy", "medium", "hard"]
VALID_TOOLS = ["search", "visit", "scholar"]


@dataclass
class ValidationResult:
    """Result of validating a single example"""
    valid: bool
    errors: List[str]
    warnings: List[str]
    quality_score: float
    dimension_scores: Dict[str, float]


@dataclass
class BatchValidationResult:
    """Result of validating a batch of examples"""
    total_examples: int
    valid_examples: int
    invalid_examples: int
    average_quality_score: float
    pass_rate: float
    hudson_approval: bool  # True if avg score >= 90
    results: List[ValidationResult]
    error_summary: Dict[str, int]


def validate_example(example: dict, agent_name: str = None) -> ValidationResult:
    """
    Validate a single training example.

    Args:
        example: Dictionary with training example fields
        agent_name: Override agent name (if not in example)

    Returns:
        ValidationResult with errors, warnings, and quality score
    """
    errors = []
    warnings = []

    # Extract agent name
    if agent_name is None:
        agent_name = example.get("agent_name")

    # Required fields validation
    required_fields = [
        "task", "context", "expected_output", "tools_used",
        "difficulty", "agent_name", "task_category"
    ]

    for field in required_fields:
        if field not in example or example[field] is None:
            errors.append(f"Missing required field: {field}")
        elif isinstance(example[field], str) and len(example[field].strip()) == 0:
            errors.append(f"Empty required field: {field}")

    # If required fields missing, can't continue validation
    if errors:
        return ValidationResult(
            valid=False,
            errors=errors,
            warnings=warnings,
            quality_score=0.0,
            dimension_scores={}
        )

    # Difficulty validation
    if example.get("difficulty") not in VALID_DIFFICULTIES:
        errors.append(
            f"Invalid difficulty: {example.get('difficulty')}. "
            f"Must be one of: {VALID_DIFFICULTIES}"
        )

    # Tools validation
    tools = example.get("tools_used", [])
    if not isinstance(tools, list):
        errors.append(f"tools_used must be a list, got: {type(tools)}")
    else:
        for tool in tools:
            if tool not in VALID_TOOLS:
                errors.append(
                    f"Invalid tool: {tool}. "
                    f"Must be one of: {VALID_TOOLS}"
                )

    # Category validation
    valid_categories = AGENT_CATEGORIES.get(agent_name, [])
    if valid_categories and example.get("task_category") not in valid_categories:
        errors.append(
            f"Invalid category '{example.get('task_category')}' for {agent_name}. "
            f"Must be one of: {valid_categories}"
        )

    # Length validation (warnings, not errors)
    context_len = len(example.get("context", ""))
    output_len = len(example.get("expected_output", ""))

    if context_len < 100:
        warnings.append(
            f"Context is too short ({context_len} chars). "
            f"Recommended: ‚â•100 chars for sufficient detail."
        )

    if output_len < 200:
        warnings.append(
            f"Expected output is too short ({output_len} chars). "
            f"Recommended: ‚â•200 chars for expert-level response."
        )

    # Task description validation
    task_len = len(example.get("task", ""))
    if task_len < 20:
        warnings.append(
            f"Task description is too short ({task_len} chars). "
            f"Should be descriptive and specific."
        )
    elif task_len > 500:
        warnings.append(
            f"Task description is very long ({task_len} chars). "
            f"Consider breaking into context field."
        )

    # Calculate quality score (Hudson's 10-dimension scoring)
    dimension_scores = calculate_dimension_scores(example)
    quality_score = sum(dimension_scores.values())

    return ValidationResult(
        valid=len(errors) == 0,
        errors=errors,
        warnings=warnings,
        quality_score=quality_score,
        dimension_scores=dimension_scores
    )


def calculate_dimension_scores(example: dict) -> Dict[str, float]:
    """
    Calculate Hudson's 10-dimension quality scoring.

    Each dimension is scored 0-10 points (100 total).

    Dimensions:
    1. Specificity - Concrete details (technologies, constraints, metrics)
    2. Realism - Real-world scenarios, not synthetic/toy examples
    3. Difficulty Accuracy - Matches labeled difficulty
    4. Context Quality - Sufficient background for success
    5. Output Quality - Demonstrates expert-level knowledge
    6. Diversity - Multiple categories, domains, technologies
    7. Actionability - Clear, unambiguous, executable
    8. Tool Usage - Realistic and appropriate
    9. Formatting - Proper structure, no syntax errors
    10. Value - Meaningful improvement to agent performance

    NOTE: This is a simplified automated scoring. Hudson will perform
    manual review on 10% sample for authoritative scoring.

    Returns:
        Dictionary mapping dimension name to score (0-10)
    """
    scores = {}

    # 1. Specificity (check for concrete details)
    task = example.get("task", "")
    context = example.get("context", "")

    specificity_keywords = [
        "POST", "GET", "API", "endpoint", "pytest", "function",
        "error", "ms", "seconds", "requests", "GB", "MB", "%",
        "users", "customers", "records", "$", "USD"
    ]
    specificity_count = sum(
        1 for keyword in specificity_keywords
        if keyword.lower() in (task + context).lower()
    )
    scores["specificity"] = min(10, specificity_count * 1.5)

    # 2. Realism (check for real-world indicators)
    realism_keywords = [
        "production", "real-world", "customer", "enterprise",
        "business", "compliance", "security", "performance",
        "scalability", "reliability"
    ]
    realism_count = sum(
        1 for keyword in realism_keywords
        if keyword.lower() in (task + context).lower()
    )
    scores["realism"] = min(10, realism_count * 2.0)

    # 3. Difficulty Accuracy (based on context length and complexity)
    difficulty = example.get("difficulty", "")
    context_len = len(context)
    output_len = len(example.get("expected_output", ""))

    if difficulty == "easy":
        # Easy tasks should be shorter and simpler
        if context_len < 300 and output_len < 500:
            scores["difficulty_accuracy"] = 9.0
        else:
            scores["difficulty_accuracy"] = 6.0
    elif difficulty == "medium":
        # Medium tasks should be moderate length
        if 300 <= context_len <= 800 and 500 <= output_len <= 1500:
            scores["difficulty_accuracy"] = 9.0
        else:
            scores["difficulty_accuracy"] = 7.0
    elif difficulty == "hard":
        # Hard tasks should be longer and more complex
        if context_len > 600 and output_len > 1000:
            scores["difficulty_accuracy"] = 9.0
        else:
            scores["difficulty_accuracy"] = 6.0
    else:
        scores["difficulty_accuracy"] = 0.0

    # 4. Context Quality (sufficient detail)
    if context_len >= 400:
        scores["context_quality"] = 10.0
    elif context_len >= 200:
        scores["context_quality"] = 8.0
    elif context_len >= 100:
        scores["context_quality"] = 6.0
    else:
        scores["context_quality"] = 3.0

    # 5. Output Quality (expert-level knowledge)
    output = example.get("expected_output", "")
    output_quality_keywords = [
        "recommendation", "best practice", "consider", "however",
        "note that", "important", "critical", "WARNING", "TIP",
        "Example:", "Step", "First", "Next", "Finally"
    ]
    output_quality_count = sum(
        1 for keyword in output_quality_keywords
        if keyword.lower() in output.lower()
    )
    scores["output_quality"] = min(10, output_quality_count * 1.5 + 3)

    # 6. Diversity (variety of technologies/domains)
    diversity_keywords = [
        "Python", "JavaScript", "Node", "Go", "Rust", "Java",
        "API", "database", "MongoDB", "PostgreSQL", "Redis",
        "AWS", "Azure", "GCP", "Docker", "Kubernetes",
        "React", "Vue", "Angular", "FastAPI", "Django"
    ]
    diversity_count = sum(
        1 for keyword in diversity_keywords
        if keyword in (task + context + output)
    )
    scores["diversity"] = min(10, diversity_count * 2.5)

    # 7. Actionability (clear and executable)
    actionability_score = 8.0  # Assume good by default
    if "?" in task and "or" in task:
        # Ambiguous tasks with choices
        actionability_score = 6.0
    if len(task.split()) < 5:
        # Too short to be clear
        actionability_score = 5.0
    scores["actionability"] = actionability_score

    # 8. Tool Usage (realistic and appropriate)
    tools = example.get("tools_used", [])
    if len(tools) == 0:
        scores["tool_usage"] = 3.0  # No tools is suspicious
    elif len(tools) > 3:
        scores["tool_usage"] = 7.0  # Too many tools might be excessive
    else:
        scores["tool_usage"] = 9.0  # 1-3 tools is typical

    # 9. Formatting (proper structure)
    formatting_score = 10.0
    if not task or not context or not output:
        formatting_score = 2.0
    elif context_len < 50 or output_len < 100:
        formatting_score = 5.0
    scores["formatting"] = formatting_score

    # 10. Value (training this will improve agent)
    # High value if it covers underrepresented scenarios
    value_score = 7.0  # Baseline

    # Higher value for complex tasks
    if difficulty == "hard":
        value_score += 1.0

    # Higher value for diverse content
    if scores["diversity"] >= 8:
        value_score += 1.0

    # Higher value for realistic scenarios
    if scores["realism"] >= 8:
        value_score += 1.0

    scores["value"] = min(10, value_score)

    return scores


def validate_batch(
    examples: List[dict],
    agent_name: str = None,
    threshold: float = 90.0
) -> BatchValidationResult:
    """
    Validate a batch of training examples.

    Args:
        examples: List of training example dictionaries
        agent_name: Agent name (if not in examples)
        threshold: Minimum quality score for Hudson approval (default: 90)

    Returns:
        BatchValidationResult with aggregate statistics
    """
    results = []
    error_counts = {}

    for example in examples:
        result = validate_example(example, agent_name=agent_name)
        results.append(result)

        # Count error types
        for error in result.errors:
            error_type = error.split(":")[0]  # Extract error category
            error_counts[error_type] = error_counts.get(error_type, 0) + 1

    # Calculate aggregate metrics
    total = len(examples)
    valid = sum(1 for r in results if r.valid)
    invalid = total - valid

    quality_scores = [r.quality_score for r in results if r.quality_score > 0]
    avg_score = statistics.mean(quality_scores) if quality_scores else 0.0

    pass_rate = (valid / total * 100) if total > 0 else 0.0
    hudson_approval = avg_score >= threshold

    return BatchValidationResult(
        total_examples=total,
        valid_examples=valid,
        invalid_examples=invalid,
        average_quality_score=avg_score,
        pass_rate=pass_rate,
        hudson_approval=hudson_approval,
        results=results,
        error_summary=error_counts
    )


def load_jsonl(file_path: Path) -> List[dict]:
    """Load training examples from JSONL file"""
    examples = []

    with open(file_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, start=1):
            line = line.strip()
            if not line:
                continue

            try:
                example = json.loads(line)
                examples.append(example)
            except json.JSONDecodeError as e:
                print(f"‚ö†Ô∏è  JSON parse error on line {line_num}: {e}")
                # Add placeholder for failed parse
                examples.append({
                    "_parse_error": str(e),
                    "_line_number": line_num
                })

    return examples


def print_batch_report(result: BatchValidationResult, verbose: bool = False):
    """Print validation report to console"""

    print("\n" + "="*80)
    print("DEEPRESEARCH QUALITY VALIDATION REPORT")
    print("="*80 + "\n")

    # Summary statistics
    print(f"üìä SUMMARY STATISTICS")
    print(f"   Total Examples:     {result.total_examples}")
    print(f"   Valid Examples:     {result.valid_examples} ({result.pass_rate:.1f}%)")
    print(f"   Invalid Examples:   {result.invalid_examples}")
    print(f"   Avg Quality Score:  {result.average_quality_score:.1f}/100")
    print(f"   Hudson Approval:    {'‚úÖ APPROVED' if result.hudson_approval else '‚ùå REJECTED'}")

    # Quality score distribution
    print(f"\nüìà QUALITY SCORE DISTRIBUTION")
    scores = [r.quality_score for r in result.results if r.quality_score > 0]
    if scores:
        print(f"   Min:  {min(scores):.1f}")
        print(f"   P25:  {statistics.quantiles(scores, n=4)[0]:.1f}")
        print(f"   P50:  {statistics.median(scores):.1f}")
        print(f"   P75:  {statistics.quantiles(scores, n=4)[2]:.1f}")
        print(f"   Max:  {max(scores):.1f}")

    # Error summary
    if result.error_summary:
        print(f"\n‚ùå ERROR SUMMARY (Top 5)")
        sorted_errors = sorted(
            result.error_summary.items(),
            key=lambda x: x[1],
            reverse=True
        )[:5]
        for error_type, count in sorted_errors:
            print(f"   {error_type}: {count} occurrences")

    # Dimension scores (average across all examples)
    print(f"\nüéØ DIMENSION SCORES (Average)")
    dimension_names = [
        "specificity", "realism", "difficulty_accuracy",
        "context_quality", "output_quality", "diversity",
        "actionability", "tool_usage", "formatting", "value"
    ]

    dimension_avgs = {}
    for dim in dimension_names:
        scores_for_dim = [
            r.dimension_scores.get(dim, 0)
            for r in result.results
            if r.dimension_scores
        ]
        if scores_for_dim:
            dimension_avgs[dim] = statistics.mean(scores_for_dim)

    for dim, avg in dimension_avgs.items():
        bar = "‚ñà" * int(avg) + "‚ñë" * (10 - int(avg))
        print(f"   {dim.replace('_', ' ').title():20} [{bar}] {avg:.1f}/10")

    # Recommendations
    print(f"\nüí° RECOMMENDATIONS")

    if result.pass_rate < 95:
        print(f"   ‚Ä¢ Fix {result.invalid_examples} invalid examples before generating more")

    if result.average_quality_score < 90:
        print(f"   ‚Ä¢ Quality score below Hudson threshold (90). Review failing dimensions:")
        for dim, avg in dimension_avgs.items():
            if avg < 7:
                print(f"     - {dim.replace('_', ' ').title()}: {avg:.1f}/10 (needs improvement)")

    if "Missing required field" in result.error_summary:
        print(f"   ‚Ä¢ {result.error_summary['Missing required field']} examples missing required fields")
        print(f"     Check JSON structure and ensure all fields are present")

    if result.hudson_approval:
        print(f"   ‚úÖ READY FOR FINE-TUNING - Quality meets Hudson approval threshold")
    else:
        print(f"   ‚ö†Ô∏è  NOT READY - Regenerate low-quality examples before fine-tuning")

    # Detailed results (if verbose)
    if verbose and result.invalid_examples > 0:
        print(f"\nüìã DETAILED INVALID EXAMPLES (First 5)")
        count = 0
        for i, result_item in enumerate(result.results):
            if not result_item.valid and count < 5:
                print(f"\n   Example {i+1}:")
                for error in result_item.errors:
                    print(f"     ‚ùå {error}")
                for warning in result_item.warnings:
                    print(f"     ‚ö†Ô∏è  {warning}")
                count += 1

    print("\n" + "="*80 + "\n")


def main():
    """Main entry point for validation script"""
    import argparse

    parser = argparse.ArgumentParser(
        description="Validate DeepResearch training examples",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    # Validate single agent file
    python scripts/validate_deepresearch_quality.py data/deepresearch_generated/qa_agent_examples.jsonl

    # Validate with custom threshold
    python scripts/validate_deepresearch_quality.py data/deepresearch_generated/qa_agent_examples.jsonl --threshold 85

    # Validate all agents
    python scripts/validate_deepresearch_quality.py --all

    # Verbose output with detailed errors
    python scripts/validate_deepresearch_quality.py qa_agent_examples.jsonl --verbose
        """
    )

    parser.add_argument(
        "file",
        nargs="?",
        type=str,
        help="Path to JSONL file with training examples"
    )
    parser.add_argument(
        "--agent",
        type=str,
        help="Agent name (if not in examples)"
    )
    parser.add_argument(
        "--threshold",
        type=float,
        default=90.0,
        help="Minimum quality score for Hudson approval (default: 90)"
    )
    parser.add_argument(
        "--all",
        action="store_true",
        help="Validate all agent files in data/deepresearch_generated/"
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Show detailed errors for invalid examples"
    )

    args = parser.parse_args()

    if args.all:
        # Validate all agents
        data_dir = Path("data/deepresearch_generated")
        if not data_dir.exists():
            print(f"‚ùå Directory not found: {data_dir}")
            print(f"   Generate examples first before validating")
            sys.exit(1)

        jsonl_files = list(data_dir.glob("*_examples.jsonl"))
        if not jsonl_files:
            print(f"‚ùå No JSONL files found in {data_dir}")
            sys.exit(1)

        print(f"üìÇ Found {len(jsonl_files)} agent files to validate\n")

        all_results = []
        for file_path in sorted(jsonl_files):
            agent_name = file_path.stem.replace("_examples", "")
            print(f"Validating {agent_name}...")

            examples = load_jsonl(file_path)
            result = validate_batch(
                examples,
                agent_name=agent_name,
                threshold=args.threshold
            )
            all_results.append((agent_name, result))

        # Print aggregate report
        print("\n" + "="*80)
        print("AGGREGATE REPORT - ALL AGENTS")
        print("="*80 + "\n")

        for agent_name, result in all_results:
            status = "‚úÖ" if result.hudson_approval else "‚ùå"
            print(
                f"{status} {agent_name:20} "
                f"{result.valid_examples}/{result.total_examples} valid "
                f"({result.pass_rate:.1f}%) "
                f"Quality: {result.average_quality_score:.1f}/100"
            )

        # Overall statistics
        total_examples = sum(r.total_examples for _, r in all_results)
        total_valid = sum(r.valid_examples for _, r in all_results)
        avg_quality = statistics.mean([r.average_quality_score for _, r in all_results])

        print(f"\nüìä OVERALL")
        print(f"   Total Examples: {total_examples}")
        print(f"   Valid Examples: {total_valid} ({total_valid/total_examples*100:.1f}%)")
        print(f"   Avg Quality:    {avg_quality:.1f}/100")

        approved_count = sum(1 for _, r in all_results if r.hudson_approval)
        print(f"\nüéØ HUDSON APPROVAL: {approved_count}/{len(all_results)} agents approved")

    elif args.file:
        # Validate single file
        file_path = Path(args.file)
        if not file_path.exists():
            print(f"‚ùå File not found: {file_path}")
            sys.exit(1)

        examples = load_jsonl(file_path)
        result = validate_batch(
            examples,
            agent_name=args.agent,
            threshold=args.threshold
        )
        print_batch_report(result, verbose=args.verbose)

        # Exit code: 0 if approved, 1 if rejected
        sys.exit(0 if result.hudson_approval else 1)

    else:
        parser.print_help()
        sys.exit(1)


if __name__ == "__main__":
    main()
