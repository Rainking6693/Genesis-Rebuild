name: CI Evaluation Harness

on:
  pull_request:
    branches: [main]
    paths:
      - 'agents/**'
      - 'infrastructure/**'
      - 'benchmarks/**'
      - 'tests/**'
  workflow_dispatch:
  schedule:
    # Nightly run at midnight UTC
    - cron: '0 0 * * *'

jobs:
  evaluate:
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.12
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Download baseline results
        uses: actions/download-artifact@v4
        with:
          name: baseline-eval-results
          path: baseline/
        continue-on-error: true

      - name: Run Evaluation Harness
        id: eval
        run: |
          python scripts/run_eval_harness.py \
            --output results.json \
            --baseline baseline/results.json \
            --agents all
        timeout-minutes: 8
        continue-on-error: true

      - name: Generate Evaluation Report
        if: always()
        run: |
          python scripts/generate_eval_report.py \
            results.json \
            baseline/results.json \
            > report.md

      - name: Post Report to PR
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            // Read report
            let report = '';
            try {
              report = fs.readFileSync('report.md', 'utf8');
            } catch (error) {
              report = '⚠️ Evaluation report generation failed. Check workflow logs.';
            }

            // Post comment
            await github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: report
            });

      - name: Check for Regressions
        run: |
          python scripts/check_regressions.py results.json

      - name: Upload results as artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: eval-results
          path: |
            results.json
            report.md
          retention-days: 30

      - name: Update baseline (main branch only)
        if: github.ref == 'refs/heads/main' && github.event_name != 'pull_request'
        uses: actions/upload-artifact@v4
        with:
          name: baseline-eval-results
          path: results.json
          retention-days: 90

  summary:
    needs: evaluate
    runs-on: ubuntu-latest
    if: always()

    steps:
      - name: Download results
        uses: actions/download-artifact@v4
        with:
          name: eval-results
          path: results/

      - name: Create job summary
        run: |
          if [ -f results/report.md ]; then
            cat results/report.md >> $GITHUB_STEP_SUMMARY
          else
            echo "⚠️ Evaluation results not available" >> $GITHUB_STEP_SUMMARY
          fi
