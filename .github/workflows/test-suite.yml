name: Test Suite

on:
  push:
    branches: [ main, develop, 'feature/**' ]
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:
  schedule:
    # Run tests daily at 2 AM UTC to catch environmental drift
    - cron: '0 2 * * *'

env:
  PYTHON_VERSION: '3.12'
  PYTEST_TIMEOUT: 600
  COVERAGE_THRESHOLD: 85
  # Security: A2A HTTPS enforcement (DO NOT set to 'true' in production/CI)
  A2A_ALLOW_HTTP: 'false'  # HTTPS required in CI/staging/production

jobs:
  # Fast syntax and linting checks
  lint:
    name: Code Quality Checks
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for better analysis

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements_infrastructure.txt
          pip install -r requirements_app.txt
          pip install ruff black isort mypy pylint pytest-cov

      - name: Run Ruff linting
        run: ruff check . --output-format=github
        continue-on-error: true

      - name: Run Black formatting check
        run: black --check --diff .
        continue-on-error: true

      - name: Run isort import check
        run: isort --check-only --diff .
        continue-on-error: true

      - name: Run type checking with mypy
        run: mypy --install-types --non-interactive --ignore-missing-imports .
        continue-on-error: true

  slice-lint:
    name: SLICE Context Lint
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: lint

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[test]"

      - name: Run SLICE context linter
        run: python scripts/run_slice_lint.py

  # OCR Regression Tests - REQUIRED for production deployment
  ocr-regression:
    name: OCR Regression Tests
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: [lint, slice-lint]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install Tesseract OCR
        run: |
          sudo apt-get update
          sudo apt-get install -y tesseract-ocr
          tesseract --version

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytesseract python-Levenshtein pillow

      - name: Run OCR regression tests
        run: |
          pytest tests/test_ocr_regression.py \
            -v \
            --tb=short \
            --junit-xml=test-results-ocr-regression.xml
        continue-on-error: false  # FAIL CI if OCR accuracy drops

      - name: Upload OCR test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results-ocr-regression
          path: test-results-ocr-regression.xml

  # Unit tests - fast, isolated tests
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: [lint, slice-lint, ocr-regression]  # OCR & SLICE must pass first

    strategy:
      fail-fast: false
      matrix:
        test-group:
          - infrastructure
          - agents
          - orchestration
          - integration

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y tesseract-ocr
          tesseract --version

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements_infrastructure.txt
          pip install -r requirements_app.txt
          pip install pytest-cov pytest-xdist pytest-timeout
          pip install pytesseract python-Levenshtein  # OCR regression tests
          pip install playwright supabase
          playwright install chromium

      - name: Run unit tests - ${{ matrix.test-group }}
        env:
          PYTEST_TIMEOUT: ${{ env.PYTEST_TIMEOUT }}
        run: |
          pytest tests/ \
            -m "not performance and not slow" \
            -k "${{ matrix.test-group }}" \
            --cov=. \
            --cov-report=xml \
            --cov-report=term-missing \
            --junit-xml=test-results-${{ matrix.test-group }}.xml \
            -v \
            --timeout=300 \
            --tb=short

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        with:
          files: ./coverage.xml
          flags: unit-${{ matrix.test-group }}
          name: unit-${{ matrix.test-group }}
          fail_ci_if_error: false

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results-unit-${{ matrix.test-group }}
          path: test-results-${{ matrix.test-group }}.xml

  # Performance tests - separate to avoid impacting unit test speed
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: unit-tests

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements_infrastructure.txt
          pip install -r requirements_app.txt
          pip install pytest pytest-benchmark pytest-timeout

      - name: Run performance tests
        run: |
          pytest tests/ \
            -m "performance" \
            --benchmark-only \
            --benchmark-autosave \
            --benchmark-save-data \
            -v \
            --timeout=600 \
            --tb=short

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: benchmark-results
          path: .benchmarks/

  # Integration tests - slower tests that require full system
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    timeout-minutes: 40
    needs: unit-tests

    strategy:
      fail-fast: false
      matrix:
        test-suite:
          - orchestration
          - darwin
          - swarm
          - error-handling

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements_infrastructure.txt
          pip install -r requirements_app.txt
          pip install pytest-cov pytest-timeout pytest-xdist
          pip install playwright supabase
          playwright install chromium

      - name: Run integration tests - ${{ matrix.test-suite }}
        env:
          PYTEST_TIMEOUT: ${{ env.PYTEST_TIMEOUT }}
        run: |
          pytest tests/ \
            -m "integration" \
            -k "${{ matrix.test-suite }}" \
            --cov=. \
            --cov-report=xml \
            --junit-xml=test-results-integration-${{ matrix.test-suite }}.xml \
            -v \
            --timeout=600 \
            --tb=short

      - name: Upload coverage
        uses: codecov/codecov-action@v4
        with:
          files: ./coverage.xml
          flags: integration-${{ matrix.test-suite }}
          name: integration-${{ matrix.test-suite }}
          fail_ci_if_error: false

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results-integration-${{ matrix.test-suite }}
          path: test-results-integration-${{ matrix.test-suite }}.xml

  # Coverage analysis and enforcement
  coverage:
    name: Coverage Analysis
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: [unit-tests, integration-tests]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements_infrastructure.txt
          pip install -r requirements_app.txt
          pip install pytest-cov coverage
          pip install playwright supabase
          playwright install chromium

      - name: Run full test suite with coverage
        run: |
          pytest tests/ \
            -m "not performance" \
            --cov=. \
            --cov-report=html \
            --cov-report=xml \
            --cov-report=term-missing \
            --cov-fail-under=${{ env.COVERAGE_THRESHOLD }} \
            -v

      - name: Generate coverage badge
        run: |
          coverage json
          python -c "
          import json
          with open('coverage.json') as f:
              data = json.load(f)
              percent = data['totals']['percent_covered']
              print(f'Coverage: {percent:.1f}%')
              with open('coverage.txt', 'w') as out:
                  out.write(f'{percent:.1f}')
          "

      - name: Upload coverage reports
        uses: actions/upload-artifact@v4
        with:
          name: coverage-report
          path: |
            htmlcov/
            coverage.xml
            coverage.json
            coverage.txt

      - name: Comment coverage on PR
        if: github.event_name == 'pull_request'
        uses: py-cov-action/python-coverage-comment-action@v3
        with:
          GITHUB_TOKEN: ${{ github.token }}
          MINIMUM_GREEN: ${{ env.COVERAGE_THRESHOLD }}
          MINIMUM_ORANGE: 70

  # GUI Benchmark Tests - OSWorld & WebArena validation
  benchmark-gui:
    name: GUI Benchmark Tests (OSWorld & WebArena)
    runs-on: ubuntu-latest
    timeout-minutes: 60
    needs: [unit-tests, ocr-regression]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            python3-tk \
            python3-dev \
            git \
            wget \
            curl \
            build-essential \
            xvfb

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements_infrastructure.txt
          pip install -r requirements_app.txt
          pip install pytest pytest-asyncio pytest-timeout playwright
          playwright install chromium firefox

      - name: Install OSWorld
        run: |
          bash scripts/install_osworld.sh || echo "OSWorld installation skipped (optional)"
        continue-on-error: true

      - name: Install WebArena
        run: |
          bash scripts/install_webarena.sh || echo "WebArena installation skipped (optional)"
        continue-on-error: true

      - name: Run OSWorld Benchmarks
        run: |
          pytest tests/test_osworld_benchmark.py \
            -v \
            -m benchmark \
            --timeout=600 \
            --tb=short \
            --junit-xml=test-results-osworld.xml
        continue-on-error: true  # Allow failures for now (benchmarks may require full setup)

      - name: Run WebArena Benchmarks
        run: |
          pytest tests/test_webarena_benchmark.py \
            -v \
            -m benchmark \
            --timeout=600 \
            --tb=short \
            --junit-xml=test-results-webarena.xml
        continue-on-error: true  # Allow failures for now (benchmarks may require full setup)

      - name: Upload Benchmark Reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: benchmark-gui-results
          path: |
            test-results-osworld.xml
            test-results-webarena.xml
            benchmark_results/

      - name: Comment Benchmark Results on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            let comment = '## GUI Benchmark Results\n\n';

            // Read OSWorld results if available
            if (fs.existsSync('benchmark_results')) {
              const files = fs.readdirSync('benchmark_results');
              const osworldFile = files.find(f => f.startsWith('osworld_results_'));
              const webarenaFile = files.find(f => f.startsWith('webarena_results_'));

              if (osworldFile) {
                const data = JSON.parse(fs.readFileSync(`benchmark_results/${osworldFile}`));
                comment += `### OSWorld Benchmark\n`;
                comment += `- Success Rate: ${(data.success_rate * 100).toFixed(1)}%\n`;
                comment += `- Tasks: ${data.passed_tasks}/${data.total_tasks}\n`;
                comment += `- Total Time: ${data.total_time.toFixed(2)}s\n\n`;
              }

              if (webarenaFile) {
                const data = JSON.parse(fs.readFileSync(`benchmark_results/${webarenaFile}`));
                comment += `### WebArena Benchmark\n`;
                comment += `- Success Rate: ${(data.success_rate * 100).toFixed(1)}%\n`;
                comment += `- Tasks: ${data.passed_tasks}/${data.total_tasks}\n`;
                comment += `- Total Time: ${data.total_time.toFixed(2)}s\n\n`;
              }
            }

            comment += '\n*Note: Benchmarks require full Computer Use implementation for >90% success rate*';

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

  # Security scanning
  security:
    name: Security Scan
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install bandit safety

      - name: Run Bandit security scan
        run: bandit -r . -f json -o bandit-report.json
        continue-on-error: true

      - name: Run Safety dependency scan
        run: safety check --json --output safety-report.json
        continue-on-error: true

      - name: Upload security reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: security-reports
          path: |
            bandit-report.json
            safety-report.json

  # Test result aggregation and status
  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: [unit-tests, integration-tests, performance-tests, coverage, security, benchmark-gui]
    if: always()

    steps:
      - name: Download all test results
        uses: actions/download-artifact@v4
        with:
          pattern: test-results-*
          path: test-results/

      - name: Publish test results
        uses: EnricoMi/publish-unit-test-result-action@v2
        if: always()
        with:
          files: |
            test-results/**/*.xml
          check_name: Test Results Summary
          comment_mode: always

      - name: Test status check
        run: |
          echo "All test jobs completed"
          echo "Check individual job results for detailed status"
