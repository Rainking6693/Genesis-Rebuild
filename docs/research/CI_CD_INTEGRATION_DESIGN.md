# CI/CD Integration Design for Rogue Automated Testing

**Date:** November 1, 2025
**Author:** Forge (Testing Agent)
**Purpose:** Complete CI/CD pipeline design for 1,500+ Rogue test scenarios
**Status:** Ready for Implementation

---

## Executive Summary

This document provides a production-ready CI/CD pipeline design for automating Rogue testing in Genesis. The pipeline integrates with GitHub Actions, enforces quality gates (95% pass rate, zero P0 failures), and provides comprehensive observability through Prometheus/Grafana/Alertmanager.

**Key Features:**
- Multi-stage testing (P0 critical, P1 important, P2 extended)
- Parallel execution (5X speedup)
- Smart caching (90% speedup on cache hits)
- Cost tracking ($72-90 per 1,500 scenarios)
- Automatic PR comments with test results
- Prometheus metrics export
- Slack/email alerting on failures
- Progressive deployment support

---

## Section 1: GitHub Actions Setup

### Workflow File Location

`.github/workflows/rogue-tests.yml`

### Complete Production Workflow

```yaml
name: Rogue Automated Testing

on:
  # Trigger on push to main branches
  push:
    branches:
      - main
      - develop
      - 'release/**'

  # Trigger on pull requests
  pull_request:
    branches:
      - main
      - develop

  # Nightly comprehensive testing
  schedule:
    - cron: '0 2 * * *'  # 2 AM UTC daily

  # Manual workflow dispatch
  workflow_dispatch:
    inputs:
      priority:
        description: 'Test priority filter (P0/P1/P2/all)'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - P0
          - P1
          - P2
      agent:
        description: 'Specific agent to test (or "all")'
        required: false
        default: 'all'
        type: string
      max_workers:
        description: 'Number of parallel workers'
        required: false
        default: '5'
        type: number
      enable_cache:
        description: 'Enable scenario caching'
        required: false
        default: true
        type: boolean

# Global environment variables
env:
  MISTRAL_API_KEY: ${{ secrets.MISTRAL_API_KEY }}
  ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
  OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
  GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
  PYTHON_VERSION: '3.12'
  NODE_VERSION: '20'
  A2A_SERVICE_PORT: 8000
  ROGUE_SERVER_PORT: 8001

jobs:
  # Job 1: P0 Critical Tests (Fast fail, blocking)
  p0-critical-tests:
    name: P0 Critical Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Cache Python dependencies
        uses: actions/cache@v3
        with:
          path: |
            ~/.cache/pip
            venv/
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip setuptools wheel
          pip install -r requirements.txt
          pip install uvx
          uvx install rogue-ai

      - name: Verify installations
        run: |
          python --version
          pip list | grep -E "(rogue|anthropic|openai)"
          uvx --version

      - name: Start Genesis A2A Service
        run: |
          python a2a_service.py > a2a_service.log 2>&1 &
          echo $! > a2a_service.pid
          sleep 10

      - name: Verify A2A Service Health
        run: |
          curl -f http://localhost:${{ env.A2A_SERVICE_PORT }}/health || {
            echo "A2A service health check failed"
            cat a2a_service.log
            exit 1
          }

          # Verify all 15 agent cards are accessible
          for agent in qa_agent support_agent legal_agent analyst_agent content_agent \
                       builder_agent deploy_agent monitor_agent security_agent finance_agent \
                       hr_agent marketing_agent sales_agent seo_agent design_agent; do
            curl -f http://localhost:${{ env.A2A_SERVICE_PORT }}/a2a/agents/$agent/card || {
              echo "Failed to access $agent card"
              exit 1
            }
          done

          echo "‚úÖ All 15 agent cards verified"

      - name: Run P0 Critical Tests
        id: p0_tests
        timeout-minutes: 25
        run: |
          python infrastructure/testing/rogue_runner.py \
            --scenarios tests/rogue/scenarios/ \
            --priority P0 \
            --output reports/rogue_p0_results.json \
            --output-md reports/rogue_p0_results.md \
            --max-workers 5 \
            --cache-enabled \
            --cost-tracking \
            --early-terminate \
            --verbose

      - name: Check P0 Pass Rate
        id: p0_check
        run: |
          python scripts/check_rogue_results.py \
            --results reports/rogue_p0_results.json \
            --threshold 98 \
            --fail-on-p0 \
            --priority P0 \
            --output reports/p0_check.json

      - name: Generate P0 Coverage Report
        if: always()
        run: |
          python scripts/generate_test_coverage_report.py \
            --results reports/rogue_p0_results.json \
            --output reports/coverage_p0.md \
            --include-agents

      - name: Upload P0 Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: rogue-p0-results
          path: |
            reports/rogue_p0_results.json
            reports/rogue_p0_results.md
            reports/coverage_p0.md
            reports/p0_check.json
            a2a_service.log
          retention-days: 30

      - name: Comment PR with P0 Results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('reports/rogue_p0_results.md', 'utf8');
            const check = JSON.parse(fs.readFileSync('reports/p0_check.json', 'utf8'));

            const emoji = check.passed ? '‚úÖ' : '‚ùå';
            const status = check.passed ? 'PASSED' : 'FAILED';

            const body = `## ${emoji} Rogue P0 Critical Tests - ${status}

**Pass Rate:** ${check.pass_rate}% (Threshold: 98%)
**Total Scenarios:** ${check.total}
**Passed:** ${check.passed_count}
**Failed:** ${check.failed_count}
**Cost:** $${check.cost_usd.toFixed(2)}

### Summary
${report.substring(0, 2000)}

---
*Full results available in workflow artifacts*
`;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: body
            });

      - name: Export P0 Metrics to Prometheus
        if: always()
        run: |
          python scripts/export_rogue_metrics.py \
            --results reports/rogue_p0_results.json \
            --prometheus-gateway ${{ secrets.PROMETHEUS_PUSHGATEWAY_URL }} \
            --job rogue-p0-tests \
            --instance ${{ github.run_id }} \
            --labels "priority=P0,branch=${{ github.ref_name }}"

      - name: Send Slack Notification on Failure
        if: failure()
        uses: slackapi/slack-github-action@v1
        with:
          webhook-url: ${{ secrets.SLACK_WEBHOOK_URL }}
          payload: |
            {
              "text": "‚ùå Rogue P0 Tests Failed",
              "blocks": [
                {
                  "type": "header",
                  "text": {
                    "type": "plain_text",
                    "text": "‚ùå Rogue P0 Critical Tests Failed"
                  }
                },
                {
                  "type": "section",
                  "fields": [
                    {"type": "mrkdwn", "text": "*Repository:*\n${{ github.repository }}"},
                    {"type": "mrkdwn", "text": "*Branch:*\n${{ github.ref_name }}"},
                    {"type": "mrkdwn", "text": "*Workflow:*\n<${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|View Run>"},
                    {"type": "mrkdwn", "text": "*Triggered by:*\n${{ github.actor }}"}
                  ]
                }
              ]
            }

      - name: Stop A2A Service
        if: always()
        run: |
          if [ -f a2a_service.pid ]; then
            kill $(cat a2a_service.pid) || true
            rm a2a_service.pid
          fi

      - name: Cleanup
        if: always()
        run: |
          rm -rf .rogue/cache/
          rm -f a2a_service.log

  # Job 2: P1 Important Tests (Parallel with P2, non-blocking for P0)
  p1-important-tests:
    name: P1 Important Tests
    runs-on: ubuntu-latest
    needs: p0-critical-tests
    timeout-minutes: 60

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install uvx
          uvx install rogue-ai

      - name: Start Genesis A2A Service
        run: |
          python a2a_service.py > a2a_service.log 2>&1 &
          echo $! > a2a_service.pid
          sleep 10

      - name: Verify A2A Service
        run: |
          curl -f http://localhost:${{ env.A2A_SERVICE_PORT }}/health

      - name: Run P1 Important Tests
        timeout-minutes: 55
        run: |
          python infrastructure/testing/rogue_runner.py \
            --scenarios tests/rogue/scenarios/ \
            --priority P1 \
            --output reports/rogue_p1_results.json \
            --output-md reports/rogue_p1_results.md \
            --max-workers 5 \
            --cache-enabled \
            --cost-tracking

      - name: Check P1 Pass Rate
        run: |
          python scripts/check_rogue_results.py \
            --results reports/rogue_p1_results.json \
            --threshold 95 \
            --priority P1

      - name: Upload P1 Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: rogue-p1-results
          path: |
            reports/rogue_p1_results.json
            reports/rogue_p1_results.md
          retention-days: 30

      - name: Export P1 Metrics
        if: always()
        run: |
          python scripts/export_rogue_metrics.py \
            --results reports/rogue_p1_results.json \
            --prometheus-gateway ${{ secrets.PROMETHEUS_PUSHGATEWAY_URL }} \
            --job rogue-p1-tests \
            --instance ${{ github.run_id }} \
            --labels "priority=P1,branch=${{ github.ref_name }}"

      - name: Stop A2A Service
        if: always()
        run: |
          if [ -f a2a_service.pid ]; then
            kill $(cat a2a_service.pid) || true
          fi

  # Job 3: P2 Extended Tests (Nightly only, non-blocking)
  p2-extended-tests:
    name: P2 Extended Tests
    runs-on: ubuntu-latest
    needs: p0-critical-tests
    if: github.event.schedule == '0 2 * * *' || github.event_name == 'workflow_dispatch'
    timeout-minutes: 90

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install uvx
          uvx install rogue-ai

      - name: Start Genesis A2A Service
        run: |
          python a2a_service.py > a2a_service.log 2>&1 &
          echo $! > a2a_service.pid
          sleep 10

      - name: Run P2 Extended Tests
        timeout-minutes: 85
        run: |
          python infrastructure/testing/rogue_runner.py \
            --scenarios tests/rogue/scenarios/ \
            --priority P2 \
            --output reports/rogue_p2_results.json \
            --output-md reports/rogue_p2_results.md \
            --max-workers 5 \
            --cache-enabled \
            --cost-tracking

      - name: Check P2 Pass Rate
        continue-on-error: true
        run: |
          python scripts/check_rogue_results.py \
            --results reports/rogue_p2_results.json \
            --threshold 90 \
            --priority P2

      - name: Upload P2 Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: rogue-p2-results
          path: |
            reports/rogue_p2_results.json
            reports/rogue_p2_results.md
          retention-days: 90

      - name: Export P2 Metrics
        if: always()
        run: |
          python scripts/export_rogue_metrics.py \
            --results reports/rogue_p2_results.json \
            --prometheus-gateway ${{ secrets.PROMETHEUS_PUSHGATEWAY_URL }} \
            --job rogue-p2-tests \
            --instance ${{ github.run_id }} \
            --labels "priority=P2,branch=${{ github.ref_name }}"

      - name: Stop A2A Service
        if: always()
        run: |
          if [ -f a2a_service.pid ]; then
            kill $(cat a2a_service.pid) || true
          fi

  # Job 4: Aggregate Results & Quality Gate
  aggregate-and-gate:
    name: Aggregate Results & Quality Gate
    runs-on: ubuntu-latest
    needs: [p0-critical-tests, p1-important-tests]
    if: always()
    timeout-minutes: 10

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Download P0 Results
        uses: actions/download-artifact@v4
        with:
          name: rogue-p0-results
          path: reports/p0/

      - name: Download P1 Results
        uses: actions/download-artifact@v4
        with:
          name: rogue-p1-results
          path: reports/p1/
        continue-on-error: true

      - name: Aggregate Results
        run: |
          python scripts/aggregate_rogue_results.py \
            --p0-results reports/p0/rogue_p0_results.json \
            --p1-results reports/p1/rogue_p1_results.json \
            --output reports/aggregate_results.json \
            --output-md reports/aggregate_results.md

      - name: Quality Gate Check
        id: quality_gate
        run: |
          python scripts/quality_gate_check.py \
            --aggregate-results reports/aggregate_results.json \
            --p0-threshold 98 \
            --p1-threshold 95 \
            --overall-threshold 95 \
            --max-cost-usd 100 \
            --output reports/quality_gate.json

      - name: Generate Trend Report
        run: |
          python scripts/generate_trend_report.py \
            --current-results reports/aggregate_results.json \
            --historical-dir reports/historical/ \
            --output reports/trend_report.md

      - name: Upload Aggregate Results
        uses: actions/upload-artifact@v4
        with:
          name: rogue-aggregate-results
          path: |
            reports/aggregate_results.json
            reports/aggregate_results.md
            reports/quality_gate.json
            reports/trend_report.md
          retention-days: 90

      - name: Comment PR with Aggregate Results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const aggregate = fs.readFileSync('reports/aggregate_results.md', 'utf8');
            const gate = JSON.parse(fs.readFileSync('reports/quality_gate.json', 'utf8'));

            const emoji = gate.passed ? '‚úÖ' : '‚ùå';
            const status = gate.passed ? 'PASSED' : 'FAILED';

            const body = `## ${emoji} Rogue Quality Gate - ${status}

**Overall Pass Rate:** ${gate.overall_pass_rate}%
**P0 Pass Rate:** ${gate.p0_pass_rate}% (Threshold: 98%)
**P1 Pass Rate:** ${gate.p1_pass_rate}% (Threshold: 95%)
**Total Cost:** $${gate.total_cost_usd.toFixed(2)}

${aggregate}

---
${gate.passed ? '‚úÖ **Merge Approved**' : '‚ùå **Merge Blocked - Fix failing tests**'}
`;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: body
            });

      - name: Set Quality Gate Status
        run: |
          GATE_STATUS=$(jq -r '.passed' reports/quality_gate.json)
          if [ "$GATE_STATUS" != "true" ]; then
            echo "‚ùå Quality gate failed"
            exit 1
          fi
          echo "‚úÖ Quality gate passed"

      - name: Send Summary Email
        if: github.event.schedule == '0 2 * * *'
        uses: dawidd6/action-send-mail@v3
        with:
          server_address: ${{ secrets.SMTP_SERVER }}
          server_port: ${{ secrets.SMTP_PORT }}
          username: ${{ secrets.SMTP_USERNAME }}
          password: ${{ secrets.SMTP_PASSWORD }}
          subject: "Rogue Nightly Test Results - ${{ github.ref_name }}"
          to: ${{ secrets.NOTIFICATION_EMAIL }}
          from: "Genesis CI/CD <noreply@genesis.ai>"
          body: file://reports/aggregate_results.md
          attachments: reports/aggregate_results.json

  # Job 5: Performance Benchmarking (Weekly)
  performance-benchmarks:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    if: github.event.schedule == '0 2 * * 0'  # Sunday 2 AM UTC
    timeout-minutes: 120

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install pytest-benchmark

      - name: Run Performance Benchmarks
        run: |
          python scripts/run_performance_benchmarks.py \
            --scenarios tests/rogue/scenarios/ \
            --output reports/performance_benchmarks.json \
            --baseline reports/baseline_performance.json

      - name: Check Performance Regression
        run: |
          python scripts/check_performance_regression.py \
            --current reports/performance_benchmarks.json \
            --baseline reports/baseline_performance.json \
            --threshold 10  # 10% regression allowed

      - name: Upload Benchmark Results
        uses: actions/upload-artifact@v4
        with:
          name: performance-benchmarks
          path: reports/performance_benchmarks.json
          retention-days: 365
```

---

## Section 2: Local Testing Commands

### Development Workflow

```bash
# 1. Run P0 tests only (fast feedback)
python infrastructure/testing/rogue_runner.py \
  --scenarios tests/rogue/scenarios/ \
  --priority P0 \
  --output reports/local_p0_results.json

# 2. Run tests for specific agent
python infrastructure/testing/rogue_runner.py \
  --scenarios tests/rogue/scenarios/qa_agent_p1.yaml \
  --output reports/qa_agent_results.json

# 3. Run tests by category
python infrastructure/testing/rogue_runner.py \
  --scenarios tests/rogue/scenarios/ \
  --category performance \
  --output reports/performance_results.json

# 4. Dry run (validate scenarios without execution)
python infrastructure/testing/rogue_runner.py \
  --scenarios tests/rogue/scenarios/ \
  --dry-run

# 5. Full test suite (all priorities)
python infrastructure/testing/rogue_runner.py \
  --scenarios tests/rogue/scenarios/ \
  --priority all \
  --output reports/full_results.json \
  --max-workers 5 \
  --cache-enabled

# 6. Debug mode (verbose logging)
python infrastructure/testing/rogue_runner.py \
  --scenarios tests/rogue/scenarios/qa_agent_p1.yaml \
  --verbose \
  --output reports/debug_results.json

# 7. Generate HTML report
python scripts/generate_html_report.py \
  --results reports/local_p0_results.json \
  --output reports/test_report.html
```

### Pre-commit Testing

```bash
# Run P0 tests before committing
git add .
python infrastructure/testing/rogue_runner.py \
  --scenarios tests/rogue/scenarios/ \
  --priority P0 \
  --output reports/precommit_results.json

# Check pass rate
python scripts/check_rogue_results.py \
  --results reports/precommit_results.json \
  --threshold 98 \
  --fail-on-p0

# If tests pass, commit
git commit -m "feat: implement new feature"
```

### Pre-push Testing

```bash
# Run P0 + P1 before pushing
python infrastructure/testing/rogue_runner.py \
  --scenarios tests/rogue/scenarios/ \
  --priority P0,P1 \
  --output reports/prepush_results.json

# Check aggregate pass rate
python scripts/check_rogue_results.py \
  --results reports/prepush_results.json \
  --threshold 95

# If tests pass, push
git push origin feature/new-feature
```

---

## Section 3: Monitoring & Alerts

### Prometheus Metrics

**Metrics exported by `scripts/export_rogue_metrics.py`:**

```python
# Test execution metrics
rogue_test_pass_rate{priority="P0|P1|P2", branch="main|develop"} <float>
rogue_test_total{priority="P0|P1|P2", branch="main|develop"} <int>
rogue_test_passed{priority="P0|P1|P2", branch="main|develop"} <int>
rogue_test_failed{priority="P0|P1|P2", branch="main|develop"} <int>

# Performance metrics
rogue_test_duration_seconds{priority="P0|P1|P2", branch="main|develop"} <histogram>
rogue_test_response_time_ms{agent="qa_agent|support_agent|...", priority="P0|P1|P2"} <histogram>

# Cost metrics
rogue_test_cost_usd_total{priority="P0|P1|P2", branch="main|develop"} <float>
rogue_test_cost_per_scenario_usd{priority="P0|P1|P2"} <gauge>

# Agent-specific metrics
rogue_agent_pass_rate{agent="qa_agent|support_agent|...", priority="P0|P1|P2"} <float>
rogue_agent_avg_response_time_ms{agent="qa_agent|support_agent|..."} <gauge>

# Policy violation metrics
rogue_policy_violations_total{policy="pii_leak|timeout|security", agent="..."} <counter>

# Cache metrics
rogue_cache_hit_rate{priority="P0|P1|P2"} <gauge>
rogue_cache_size_bytes{} <gauge>
```

**Implementation (`scripts/export_rogue_metrics.py`):**

```python
#!/usr/bin/env python3
"""Export Rogue test metrics to Prometheus Pushgateway."""

import json
import argparse
from prometheus_client import CollectorRegistry, Gauge, Histogram, Counter, push_to_gateway

def export_metrics(results_path: str, prometheus_gateway: str, job: str, instance: str, labels: dict):
    """Export Rogue test results to Prometheus."""

    with open(results_path, 'r') as f:
        results = json.load(f)

    registry = CollectorRegistry()

    # Test execution metrics
    pass_rate = Gauge('rogue_test_pass_rate', 'Test pass rate', labelnames=labels.keys(), registry=registry)
    pass_rate.labels(**labels).set(results['summary']['pass_rate'])

    total = Gauge('rogue_test_total', 'Total tests', labelnames=labels.keys(), registry=registry)
    total.labels(**labels).set(results['summary']['total'])

    passed = Gauge('rogue_test_passed', 'Passed tests', labelnames=labels.keys(), registry=registry)
    passed.labels(**labels).set(results['summary']['passed'])

    failed = Gauge('rogue_test_failed', 'Failed tests', labelnames=labels.keys(), registry=registry)
    failed.labels(**labels).set(results['summary']['failed'])

    # Cost metrics
    cost = Gauge('rogue_test_cost_usd_total', 'Total cost USD', labelnames=labels.keys(), registry=registry)
    cost.labels(**labels).set(results['summary']['cost_usd'])

    # Performance metrics
    duration = Histogram('rogue_test_duration_seconds', 'Test duration', labelnames=labels.keys(), registry=registry)
    for result in results['results']:
        duration.labels(**labels).observe(result['response_time_ms'] / 1000)

    # Agent-specific metrics
    agent_pass_rate = Gauge('rogue_agent_pass_rate', 'Agent pass rate', labelnames=['agent', 'priority'], registry=registry)
    agent_response_time = Gauge('rogue_agent_avg_response_time_ms', 'Agent avg response time', labelnames=['agent'], registry=registry)

    agent_stats = {}
    for result in results['results']:
        agent = result['agent']
        if agent not in agent_stats:
            agent_stats[agent] = {'total': 0, 'passed': 0, 'response_times': []}

        agent_stats[agent]['total'] += 1
        if result['status'] == 'passed':
            agent_stats[agent]['passed'] += 1
        agent_stats[agent]['response_times'].append(result['response_time_ms'])

    for agent, stats in agent_stats.items():
        pass_rate_value = (stats['passed'] / stats['total']) * 100
        agent_pass_rate.labels(agent=agent, priority=labels.get('priority', 'all')).set(pass_rate_value)

        avg_response_time = sum(stats['response_times']) / len(stats['response_times'])
        agent_response_time.labels(agent=agent).set(avg_response_time)

    # Policy violation metrics
    violations = Counter('rogue_policy_violations_total', 'Policy violations', labelnames=['policy', 'agent'], registry=registry)
    for result in results['results']:
        for violation in result.get('policy_violations', []):
            violations.labels(policy=violation, agent=result['agent']).inc()

    # Push to gateway
    push_to_gateway(prometheus_gateway, job=job, registry=registry, grouping_key={'instance': instance})
    print(f"‚úÖ Metrics exported to {prometheus_gateway} (job={job}, instance={instance})")

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--results', required=True, help='Path to results JSON')
    parser.add_argument('--prometheus-gateway', required=True, help='Prometheus Pushgateway URL')
    parser.add_argument('--job', required=True, help='Job name')
    parser.add_argument('--instance', required=True, help='Instance ID')
    parser.add_argument('--labels', help='Additional labels (key=value,key=value)')
    args = parser.parse_args()

    labels = {}
    if args.labels:
        for pair in args.labels.split(','):
            k, v = pair.split('=')
            labels[k] = v

    export_metrics(args.results, args.prometheus_gateway, args.job, args.instance, labels)
```

### Grafana Dashboard

**Dashboard JSON:** `/infrastructure/grafana/rogue_dashboard.json`

**Panels:**

1. **Overall Pass Rate (Gauge)**
   - Query: `rogue_test_pass_rate{branch="main"}`
   - Thresholds: <90% red, 90-95% yellow, >95% green

2. **Test Execution Timeline (Graph)**
   - Query: `rate(rogue_test_total[5m])`
   - Shows test execution rate over time

3. **Pass Rate by Priority (Bar Chart)**
   - Queries:
     - `rogue_test_pass_rate{priority="P0"}`
     - `rogue_test_pass_rate{priority="P1"}`
     - `rogue_test_pass_rate{priority="P2"}`

4. **Agent Performance Heatmap**
   - Query: `rogue_agent_avg_response_time_ms`
   - Color scale: <1s green, 1-2s yellow, >2s red

5. **Cost Tracking (Graph)**
   - Query: `rogue_test_cost_usd_total`
   - Shows cumulative cost over time

6. **Policy Violations (Counter)**
   - Query: `sum by (policy) (rogue_policy_violations_total)`
   - Shows violations by type

7. **Cache Hit Rate (Gauge)**
   - Query: `rogue_cache_hit_rate`
   - Target: >90%

### Alertmanager Rules

**File:** `/infrastructure/prometheus/alerts.yml`

```yaml
groups:
  - name: rogue_tests
    interval: 30s
    rules:
      # Alert on low pass rate
      - alert: RogueTestPassRateLow
        expr: rogue_test_pass_rate{priority="P0"} < 98
        for: 5m
        labels:
          severity: critical
          team: qa
        annotations:
          summary: "Rogue P0 pass rate below 98%"
          description: "P0 test pass rate is {{ $value }}% (threshold: 98%)"

      - alert: RogueTestPassRateLowP1
        expr: rogue_test_pass_rate{priority="P1"} < 95
        for: 10m
        labels:
          severity: warning
          team: qa
        annotations:
          summary: "Rogue P1 pass rate below 95%"
          description: "P1 test pass rate is {{ $value }}% (threshold: 95%)"

      # Alert on high failure count
      - alert: RogueTestHighFailureCount
        expr: rogue_test_failed{priority="P0"} > 5
        for: 5m
        labels:
          severity: critical
          team: qa
        annotations:
          summary: "High P0 test failure count"
          description: "{{ $value }} P0 tests failing (threshold: 5)"

      # Alert on slow response times
      - alert: RogueAgentSlowResponseTime
        expr: rogue_agent_avg_response_time_ms > 3000
        for: 10m
        labels:
          severity: warning
          team: performance
        annotations:
          summary: "Agent {{ $labels.agent }} slow response time"
          description: "Average response time {{ $value }}ms (threshold: 3000ms)"

      # Alert on high cost
      - alert: RogueTestCostHigh
        expr: rogue_test_cost_usd_total > 150
        for: 5m
        labels:
          severity: warning
          team: finance
        annotations:
          summary: "Rogue test cost exceeding budget"
          description: "Test cost ${{ $value }} (threshold: $150)"

      # Alert on policy violations
      - alert: RogueSecurityPolicyViolation
        expr: increase(rogue_policy_violations_total{policy=~"pii_leak|security"}[5m]) > 0
        for: 1m
        labels:
          severity: critical
          team: security
        annotations:
          summary: "Security policy violation detected"
          description: "{{ $labels.policy }} violation in {{ $labels.agent }}"

      # Alert on low cache hit rate
      - alert: RogueCacheHitRateLow
        expr: rogue_cache_hit_rate < 0.80
        for: 15m
        labels:
          severity: info
          team: performance
        annotations:
          summary: "Rogue cache hit rate low"
          description: "Cache hit rate {{ $value }} (target: >0.90)"
```

### Slack Notifications

**Slack Webhook Integration:**

```yaml
# .github/workflows/rogue-tests.yml (Slack notification step)
- name: Send Slack Notification
  if: failure() || success()
  uses: slackapi/slack-github-action@v1
  with:
    webhook-url: ${{ secrets.SLACK_WEBHOOK_URL }}
    payload: |
      {
        "text": "${{ job.status == 'success' && '‚úÖ' || '‚ùå' }} Rogue Tests ${{ job.status }}",
        "blocks": [
          {
            "type": "header",
            "text": {
              "type": "plain_text",
              "text": "${{ job.status == 'success' && '‚úÖ' || '‚ùå' }} Rogue Tests ${{ job.status }}"
            }
          },
          {
            "type": "section",
            "fields": [
              {"type": "mrkdwn", "text": "*Repository:*\n${{ github.repository }}"},
              {"type": "mrkdwn", "text": "*Branch:*\n${{ github.ref_name }}"},
              {"type": "mrkdwn", "text": "*Workflow:*\n<${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|View Run>"},
              {"type": "mrkdwn", "text": "*Triggered by:*\n${{ github.actor }}"},
              {"type": "mrkdwn", "text": "*Pass Rate:*\n{{ pass_rate }}%"},
              {"type": "mrkdwn", "text": "*Cost:*\n${{ cost }}"}
            ]
          },
          {
            "type": "section",
            "text": {
              "type": "mrkdwn",
              "text": "View full results in workflow artifacts"
            },
            "accessory": {
              "type": "button",
              "text": {"type": "plain_text", "text": "View Artifacts"},
              "url": "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
            }
          }
        ]
      }
```

---

## Section 4: Deployment Integration

### Progressive Rollout Support

**Strategy:** Use Rogue tests as gates for progressive deployment (0% ‚Üí 25% ‚Üí 50% ‚Üí 100%)

```yaml
# .github/workflows/progressive-deploy.yml
name: Progressive Deployment with Rogue Gates

on:
  workflow_dispatch:
    inputs:
      rollout_percentage:
        description: 'Target rollout percentage (0/25/50/100)'
        required: true
        type: choice
        options:
          - '0'
          - '25'
          - '50'
          - '100'

jobs:
  rogue-gate:
    name: Rogue Quality Gate
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      # [Install dependencies, start A2A service]

      - name: Run Rogue Tests
        run: |
          python infrastructure/testing/rogue_runner.py \
            --scenarios tests/rogue/scenarios/ \
            --priority P0,P1 \
            --output reports/deployment_gate_results.json

      - name: Check Quality Gate
        run: |
          python scripts/check_rogue_results.py \
            --results reports/deployment_gate_results.json \
            --threshold 95 \
            --fail-on-p0

      - name: Set Deployment Approval
        id: approval
        run: |
          echo "approved=true" >> $GITHUB_OUTPUT

  deploy:
    name: Deploy to Production
    needs: rogue-gate
    runs-on: ubuntu-latest

    steps:
      - name: Deploy to ${{ github.event.inputs.rollout_percentage }}%
        run: |
          ./scripts/progressive_deploy.sh \
            --percentage ${{ github.event.inputs.rollout_percentage }} \
            --environment production

      - name: Wait for Deployment
        run: sleep 300  # 5 minutes

      - name: Run Smoke Tests
        run: |
          python infrastructure/testing/rogue_runner.py \
            --scenarios tests/rogue/scenarios/smoke_tests.yaml \
            --output reports/smoke_test_results.json

      - name: Check Smoke Tests
        run: |
          python scripts/check_rogue_results.py \
            --results reports/smoke_test_results.json \
            --threshold 100 \
            --fail-on-p0

      - name: Rollback on Failure
        if: failure()
        run: |
          ./scripts/rollback_deployment.sh \
            --environment production
```

### Post-Deployment Validation

**Run subset of critical tests after deployment:**

```yaml
# .github/workflows/post-deploy-validation.yml
name: Post-Deployment Validation

on:
  deployment_status:

jobs:
  post-deploy-tests:
    if: github.event.deployment_status.state == 'success'
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Run Critical Path Tests
        run: |
          python infrastructure/testing/rogue_runner.py \
            --scenarios tests/rogue/scenarios/ \
            --tags critical,smoke \
            --output reports/post_deploy_results.json

      - name: Check Results
        run: |
          python scripts/check_rogue_results.py \
            --results reports/post_deploy_results.json \
            --threshold 100

      - name: Alert on Failure
        if: failure()
        run: |
          # Send PagerDuty alert
          curl -X POST https://events.pagerduty.com/v2/enqueue \
            -H "Authorization: Token ${{ secrets.PAGERDUTY_TOKEN }}" \
            -d '{
              "event_action": "trigger",
              "payload": {
                "summary": "Post-deployment tests failed",
                "severity": "critical",
                "source": "rogue-tests"
              }
            }'
```

---

## Section 5: Helper Scripts

### Script 1: `scripts/check_rogue_results.py`

```python
#!/usr/bin/env python3
"""Check Rogue test results against quality gates."""

import json
import sys
import argparse
from typing import Dict, Any

def check_results(
    results_path: str,
    threshold: float,
    fail_on_p0: bool,
    priority: str = None,
    output_path: str = None
) -> bool:
    """
    Check Rogue test results against blocking criteria.

    Args:
        results_path: Path to results JSON
        threshold: Pass rate threshold (%)
        fail_on_p0: Block on P0 failures
        priority: Filter by priority (P0/P1/P2)
        output_path: Optional output JSON path

    Returns:
        True if all checks pass, False otherwise
    """
    with open(results_path, 'r') as f:
        results = json.load(f)

    checks = {
        'passed': True,
        'pass_rate': results['summary']['pass_rate'],
        'total': results['summary']['total'],
        'passed_count': results['summary']['passed'],
        'failed_count': results['summary']['failed'],
        'cost_usd': results['summary']['cost_usd'],
        'failures': []
    }

    # Check 1: Pass rate ‚â• threshold
    if results['summary']['pass_rate'] < threshold:
        print(f"‚ùå FAIL: Pass rate {results['summary']['pass_rate']:.1f}% below threshold {threshold}%")
        checks['passed'] = False
    else:
        print(f"‚úÖ PASS: Pass rate {results['summary']['pass_rate']:.1f}% meets threshold {threshold}%")

    # Check 2: No P0 failures (if enabled)
    if fail_on_p0:
        p0_failures = [
            r for r in results['results']
            if r.get('priority') == 'P0' and r['status'] == 'failed'
        ]
        if p0_failures:
            print(f"‚ùå FAIL: {len(p0_failures)} P0 scenario(s) failed:")
            for failure in p0_failures[:5]:
                print(f"  - {failure['scenario_id']}: {failure.get('rationale', 'No rationale')}")
                checks['failures'].append({
                    'id': failure['scenario_id'],
                    'priority': 'P0',
                    'rationale': failure.get('rationale', 'No rationale')
                })
            checks['passed'] = False
        else:
            print(f"‚úÖ PASS: All P0 scenarios passed")

    # Check 3: Average response time <2s (warning only)
    avg_response_time = results['summary'].get('avg_response_time_ms', 0) / 1000
    if avg_response_time >= 2.0:
        print(f"‚ö†Ô∏è  WARNING: Average response time {avg_response_time:.2f}s exceeds 2s (not blocking)")
    else:
        print(f"‚úÖ PASS: Average response time {avg_response_time:.2f}s meets threshold")

    # Check 4: Zero security violations
    security_violations = [
        r for r in results['results']
        if any('security' in v.lower() or 'pii' in v.lower() for v in r.get('policy_violations', []))
    ]
    if security_violations:
        print(f"‚ùå FAIL: {len(security_violations)} security violation(s) detected:")
        for violation in security_violations[:3]:
            print(f"  - {violation['scenario_id']}: {violation['policy_violations']}")
        checks['passed'] = False
    else:
        print(f"‚úÖ PASS: No security violations detected")

    # Output results
    if output_path:
        with open(output_path, 'w') as f:
            json.dump(checks, f, indent=2)

    if checks['passed']:
        print("\nüéâ All checks passed - merge approved")
    else:
        print("\n‚ùå Quality gate failed - fix issues before merging")

    return checks['passed']

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--results', required=True, help='Path to results JSON')
    parser.add_argument('--threshold', type=float, default=95, help='Pass rate threshold')
    parser.add_argument('--fail-on-p0', action='store_true', help='Block on P0 failures')
    parser.add_argument('--priority', help='Filter by priority (P0/P1/P2)')
    parser.add_argument('--output', help='Output JSON path')
    args = parser.parse_args()

    success = check_results(
        args.results,
        args.threshold,
        args.fail_on_p0,
        args.priority,
        args.output
    )

    sys.exit(0 if success else 1)
```

### Script 2: `scripts/aggregate_rogue_results.py`

```python
#!/usr/bin/env python3
"""Aggregate Rogue test results from multiple runs."""

import json
import argparse
from typing import List, Dict, Any
from pathlib import Path

def aggregate_results(p0_path: str, p1_path: str = None, p2_path: str = None) -> Dict[str, Any]:
    """Aggregate results from P0, P1, P2 runs."""

    aggregated = {
        'summary': {
            'total': 0,
            'passed': 0,
            'failed': 0,
            'pass_rate': 0.0,
            'cost_usd': 0.0,
            'avg_response_time_ms': 0.0
        },
        'by_priority': {},
        'by_agent': {},
        'results': []
    }

    # Load all results
    for path, priority in [(p0_path, 'P0'), (p1_path, 'P1'), (p2_path, 'P2')]:
        if not path or not Path(path).exists():
            continue

        with open(path, 'r') as f:
            data = json.load(f)

        # Aggregate summary
        aggregated['summary']['total'] += data['summary']['total']
        aggregated['summary']['passed'] += data['summary']['passed']
        aggregated['summary']['failed'] += data['summary']['failed']
        aggregated['summary']['cost_usd'] += data['summary']['cost_usd']

        # Store by priority
        aggregated['by_priority'][priority] = {
            'total': data['summary']['total'],
            'passed': data['summary']['passed'],
            'failed': data['summary']['failed'],
            'pass_rate': data['summary']['pass_rate']
        }

        # Aggregate by agent
        for result in data['results']:
            agent = result['agent']
            if agent not in aggregated['by_agent']:
                aggregated['by_agent'][agent] = {
                    'total': 0,
                    'passed': 0,
                    'failed': 0,
                    'response_times': []
                }

            aggregated['by_agent'][agent]['total'] += 1
            if result['status'] == 'passed':
                aggregated['by_agent'][agent]['passed'] += 1
            else:
                aggregated['by_agent'][agent]['failed'] += 1

            aggregated['by_agent'][agent]['response_times'].append(result['response_time_ms'])

        # Append all results
        aggregated['results'].extend(data['results'])

    # Calculate overall pass rate
    if aggregated['summary']['total'] > 0:
        aggregated['summary']['pass_rate'] = (
            aggregated['summary']['passed'] / aggregated['summary']['total']
        ) * 100

    # Calculate average response time
    all_response_times = [r['response_time_ms'] for r in aggregated['results']]
    if all_response_times:
        aggregated['summary']['avg_response_time_ms'] = sum(all_response_times) / len(all_response_times)

    # Calculate per-agent pass rates
    for agent, stats in aggregated['by_agent'].items():
        stats['pass_rate'] = (stats['passed'] / stats['total']) * 100 if stats['total'] > 0 else 0
        stats['avg_response_time_ms'] = sum(stats['response_times']) / len(stats['response_times'])

    return aggregated

def generate_markdown_report(aggregated: Dict[str, Any]) -> str:
    """Generate Markdown report from aggregated results."""

    report = f"""# Rogue Test Results - Aggregated Report

## Summary

**Overall Pass Rate:** {aggregated['summary']['pass_rate']:.1f}% ({aggregated['summary']['passed']}/{aggregated['summary']['total']})
**Total Cost:** ${aggregated['summary']['cost_usd']:.2f}
**Average Response Time:** {aggregated['summary']['avg_response_time_ms']:.0f}ms

## Results by Priority

| Priority | Total | Passed | Failed | Pass Rate |
|----------|-------|--------|--------|-----------|
"""

    for priority in ['P0', 'P1', 'P2']:
        if priority in aggregated['by_priority']:
            p = aggregated['by_priority'][priority]
            report += f"| {priority} | {p['total']} | {p['passed']} | {p['failed']} | {p['pass_rate']:.1f}% |\n"

    report += "\n## Results by Agent\n\n"
    report += "| Agent | Total | Passed | Failed | Pass Rate | Avg Response Time |\n"
    report += "|-------|-------|--------|--------|-----------|-------------------|\n"

    for agent in sorted(aggregated['by_agent'].keys()):
        a = aggregated['by_agent'][agent]
        report += f"| {agent} | {a['total']} | {a['passed']} | {a['failed']} | {a['pass_rate']:.1f}% | {a['avg_response_time_ms']:.0f}ms |\n"

    # List failed scenarios
    failed_scenarios = [r for r in aggregated['results'] if r['status'] == 'failed']
    if failed_scenarios:
        report += "\n## Failed Scenarios\n\n"
        for scenario in failed_scenarios[:20]:  # Show first 20
            report += f"### {scenario['scenario_id']} ({scenario.get('priority', 'P?')})\n"
            report += f"**Agent:** {scenario['agent']}\n"
            report += f"**Rationale:** {scenario.get('rationale', 'No rationale')}\n"
            if scenario.get('policy_violations'):
                report += f"**Policy Violations:** {', '.join(scenario['policy_violations'])}\n"
            report += "\n"

    return report

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--p0-results', required=True, help='Path to P0 results JSON')
    parser.add_argument('--p1-results', help='Path to P1 results JSON')
    parser.add_argument('--p2-results', help='Path to P2 results JSON')
    parser.add_argument('--output', required=True, help='Output JSON path')
    parser.add_argument('--output-md', help='Output Markdown path')
    args = parser.parse_args()

    aggregated = aggregate_results(args.p0_results, args.p1_results, args.p2_results)

    with open(args.output, 'w') as f:
        json.dump(aggregated, f, indent=2)

    if args.output_md:
        report = generate_markdown_report(aggregated)
        with open(args.output_md, 'w') as f:
            f.write(report)

    print(f"‚úÖ Aggregated results written to {args.output}")
```

---

## Conclusion

This CI/CD integration design provides a production-ready pipeline for automating Rogue testing in Genesis. Key features include:

- ‚úÖ Multi-stage testing (P0/P1/P2) with parallel execution
- ‚úÖ Quality gates enforcing 95%+ pass rate and zero P0 failures
- ‚úÖ Comprehensive monitoring (Prometheus/Grafana/Alertmanager)
- ‚úÖ Automatic PR comments with test results
- ‚úÖ Progressive deployment support with smoke tests
- ‚úÖ Cost tracking and budget enforcement
- ‚úÖ Slack/email notifications on failures

**Next Steps:**
1. Implement GitHub Actions workflow (`.github/workflows/rogue-tests.yml`)
2. Deploy helper scripts (`check_rogue_results.py`, `aggregate_rogue_results.py`, `export_rogue_metrics.py`)
3. Configure Prometheus/Grafana dashboards
4. Set up Alertmanager rules
5. Test full pipeline on feature branch

**Expected Timeline:** 2-3 days for complete CI/CD implementation.
