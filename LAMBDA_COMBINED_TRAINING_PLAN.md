# Lambda Labs Combined Training Plan - Cross-Agent + SAE PII

**Total GPU Time Needed:** ~40-50 hours  
**Estimated Cost:** $44-55 (A100 40GB @ $1.10/hour)  
**Strategy:** Run both training jobs in parallel on separate GPUs or sequentially

---

## ðŸ“‹ What Needs Training

### 1. Cross-Agent Fine-Tuning (5 agents)
- **Data:** 99,990 examples ready in `data/training/`
- **Base Model:** Llama-3.1-8B
- **Time:** ~4 hours per agent Ã— 5 = 20 hours
- **Cost:** $22 (A100 @ $1.10/hour)

### 2. SAE PII Probe Training
- **Component A:** SAE Encoder (32,768 latents, Layer 12)
  - Dataset: LMSYS-Chat-1M
  - Time: ~8-12 hours
  - Cost: $9-13

- **Component B:** PII Classifiers (5 categories)
  - Dataset: 1,000 synthetic PII examples
  - Time: ~1-2 hours
  - Cost: $1-2

**SAE Total:** ~10-14 hours, $10-15

---

## ðŸŽ¯ Combined Execution Strategy

### Option 1: Sequential (Recommended for Single GPU)

**Run on 1Ã— A100 (40GB):**
```
Day 1-2: Cross-Agent Fine-Tuning (20 hours)
  â†’ Fine-tune all 5 agents sequentially
  
Day 2-3: SAE Training (10-14 hours)
  â†’ Train SAE encoder
  â†’ Train PII classifiers
  
Total: 30-34 hours
Cost: $33-37 (A100 @ $1.10/hour)
```

### Option 2: Parallel (Faster with 2 GPUs)

**Run on 2Ã— A100 (40GB each):**
```
GPU 1: Cross-Agent Fine-Tuning (20 hours)
  â†’ All 5 agents

GPU 2: SAE Training (10-14 hours)
  â†’ SAE encoder + PII classifiers
  
Total: 20 hours (parallel)
Cost: $22 + $11-15 = $33-37 (same cost, half the time!)
```

### Option 3: Budget (RTX 4090)

**Run on 1Ã— RTX 4090 (24GB):**
```
Cross-Agent: ~24 hours (slower on 24GB VRAM)
SAE Training: ~12-16 hours
Total: 36-40 hours
Cost: $14-16 (@ $0.39/hour on Runpod)

Note: May need reduced batch sizes for 24GB VRAM
```

---

## ðŸ“¦ Lambda Labs Setup

### Step 1: Launch Instance

**Go to:** https://lambdalabs.com/service/gpu-cloud

**Recommended Configuration:**
```
GPU: 1Ã— A100 (40GB SXM4)
Region: US-TX-1 (Austin) or US-CA-1 (San Jose)
Price: $1.10/hour
Storage: 200GB
Instance: pytorch-2.2 (pre-configured with PyTorch)
```

**Or for parallel:**
```
GPU: 2Ã— A100 (40GB SXM4)
Price: $2.20/hour
Total time: 20 hours (parallel)
Total cost: $44
```

### Step 2: Transfer Training Data

**From your VPS:**
```bash
cd /home/genesis/genesis-rebuild

# Package training data
tar -czf cross_agent_training.tar.gz data/training/*.jsonl

# Package scripts
tar -czf training_scripts.tar.gz \
    infrastructure/finetune/ \
    scripts/finetune*.py \
    scripts/train_sae*.py

# Transfer to Lambda (replace with your Lambda IP)
scp cross_agent_training.tar.gz ubuntu@LAMBDA_IP:/home/ubuntu/
scp training_scripts.tar.gz ubuntu@LAMBDA_IP:/home/ubuntu/
```

### Step 3: Setup on Lambda

**SSH into Lambda:**
```bash
ssh ubuntu@LAMBDA_IP

# Extract data
cd /home/ubuntu
tar -xzf cross_agent_training.tar.gz
tar -xzf training_scripts.tar.gz

# Install dependencies
pip install unsloth transformers datasets trl peft accelerate bitsandbytes
pip install anthropic  # For any data augmentation needs
```

---

## ðŸš€ Execution Plan (Single GPU Sequential)

### Day 1-2: Cross-Agent Fine-Tuning (20 hours, $22)

**Create fine-tuning script:**
```bash
cat > train_all_agents.sh << 'SCRIPT'
#!/bin/bash
set -e

echo "Starting cross-agent fine-tuning for 5 agents..."

for agent in qa_agent support_agent legal_agent analyst_agent content_agent; do
    echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
    echo "â•‘  Fine-tuning: $agent"
    echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    
    python3 << PYTHON
from unsloth import FastLanguageModel
from trl import SFTTrainer
from transformers import TrainingArguments
import json

# Load model
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="meta-llama/Llama-3.1-8B",
    max_seq_length=2048,
    load_in_4bit=True,
)

# Add LoRA adapters
model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    use_gradient_checkpointing="unsloth",
)

# Load training data
with open("data/training/${agent}_training.jsonl") as f:
    data = [json.loads(line) for line in f]

print(f"Loaded {len(data):,} training examples for ${agent}")

# Format for training
def format_example(example):
    messages = example.get("messages", [])
    return tokenizer.apply_chat_template(messages, tokenize=False)

formatted_data = [{"text": format_example(ex)} for ex in data]

from datasets import Dataset
dataset = Dataset.from_list(formatted_data)

# Training arguments
training_args = TrainingArguments(
    output_dir=f"models/${agent}_finetuned",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    learning_rate=2e-4,
    fp16=True,
    logging_steps=10,
    save_steps=500,
    warmup_steps=100,
    max_grad_norm=1.0,
)

# Train
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    dataset_text_field="text",
    max_seq_length=2048,
    args=training_args,
)

print(f"Starting training for ${agent}...")
trainer.train()

# Save model
model.save_pretrained(f"models/${agent}_finetuned")
tokenizer.save_pretrained(f"models/${agent}_finetuned")

print(f"âœ… ${agent} fine-tuning complete!")
PYTHON

    echo ""
    echo "âœ… Completed: $agent"
    echo ""
done

echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘  All 5 agents fine-tuned successfully! âœ…"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
SCRIPT

chmod +x train_all_agents.sh
```

**Run it:**
```bash
./train_all_agents.sh 2>&1 | tee cross_agent_training.log
```

### Day 2-3: SAE PII Training (10-14 hours, $11-15)

**Create SAE training script:**
```bash
cat > train_sae_pii.sh << 'SCRIPT'
#!/bin/bash
set -e

echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘  SAE PII Probe Training"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

# Part 1: Train SAE Encoder (8-12 hours)
echo "Part 1: Training SAE Encoder (32,768 latents, Layer 12)..."
python3 << PYTHON
import torch
import torch.nn as nn
from transformers import AutoModel, AutoTokenizer
from datasets import load_dataset

# Load base model
model_name = "meta-llama/Llama-3.2-8B"
model = AutoModel.from_pretrained(model_name, torch_dtype=torch.float16, device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)

# SAE Architecture
class SparseAutoencoder(nn.Module):
    def __init__(self, hidden_dim=4096, latent_dim=32768, k=64):
        super().__init__()
        self.encoder = nn.Linear(hidden_dim, latent_dim)
        self.decoder = nn.Linear(latent_dim, hidden_dim)
        self.k = k  # Top-k sparsity
        
    def forward(self, x):
        # Encode
        h = torch.relu(self.encoder(x))
        
        # Top-k sparsity constraint
        topk_values, topk_indices = torch.topk(h, self.k, dim=-1)
        h_sparse = torch.zeros_like(h)
        h_sparse.scatter_(-1, topk_indices, topk_values)
        
        # Decode
        x_recon = self.decoder(h_sparse)
        return x_recon, h_sparse

# Initialize SAE
sae = SparseAutoencoder(hidden_dim=4096, latent_dim=32768, k=64).cuda()

# Load LMSYS-Chat-1M dataset (or subset)
print("Loading training dataset...")
dataset = load_dataset("lmsys/lmsys-chat-1m", split="train[:10000]")  # Use subset for faster training

# Training loop
optimizer = torch.optim.AdamW(sae.parameters(), lr=1e-4)
num_epochs = 10

print(f"Training SAE for {num_epochs} epochs on {len(dataset)} examples...")

for epoch in range(num_epochs):
    total_loss = 0
    for i, example in enumerate(dataset):
        # Get layer 12 activations
        text = example['conversation'][0]['content'][:512]  # Truncate
        inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512).to("cuda")
        
        with torch.no_grad():
            outputs = model(**inputs, output_hidden_states=True)
            layer_12_activations = outputs.hidden_states[12].mean(dim=1)  # [batch, hidden]
        
        # Train SAE
        optimizer.zero_grad()
        recon, sparse = sae(layer_12_activations)
        
        # Loss: reconstruction + sparsity penalty
        loss_recon = nn.functional.mse_loss(recon, layer_12_activations)
        loss_sparse = sparse.abs().mean()
        loss = loss_recon + 0.1 * loss_sparse
        
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        
        if (i + 1) % 100 == 0:
            print(f"  Epoch {epoch+1}/{num_epochs}, Step {i+1}, Loss: {loss.item():.4f}")
    
    avg_loss = total_loss / len(dataset)
    print(f"âœ… Epoch {epoch+1} complete. Avg loss: {avg_loss:.4f}")

# Save SAE encoder
torch.save(sae.state_dict(), "models/sae_layer12_8x.pt")
print("âœ… SAE encoder saved to models/sae_layer12_8x.pt")
PYTHON

# Part 2: Generate PII Dataset (30 minutes)
echo ""
echo "Part 2: Generating 1,000 PII examples..."
python3 << PYTHON
from anthropic import Anthropic
import json
import os

client = Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))

pii_categories = ["personal_name", "address", "phone", "email"]
examples = []

for i in range(1000):
    category = pii_categories[i % 4]
    
    prompt = f"Generate a realistic text snippet containing {category} PII. Return JSON with 'text' and 'pii_spans' (list of {{category, start, end, text}})."
    
    response = client.messages.create(
        model="claude-haiku-4.5",
        max_tokens=500,
        messages=[{"role": "user", "content": prompt}]
    )
    
    examples.append(response.content[0].text)
    
    if (i + 1) % 100 == 0:
        print(f"  Generated {i+1}/1000 PII examples")

with open("data/pii_dataset_1k.json", "w") as f:
    json.dump(examples, f, indent=2)

print("âœ… Generated 1,000 PII examples")
PYTHON

# Part 3: Train PII Classifiers (1-2 hours)
echo ""
echo "Part 3: Training PII classifiers..."
python3 << PYTHON
import torch
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
import xgboost as xgb
import pickle
import json

# Load SAE encoder
sae = torch.load("models/sae_layer12_8x.pt")
sae.eval()

# Load PII dataset
with open("data/pii_dataset_1k.json") as f:
    pii_data = json.load(f)

# Extract SAE features and labels
print("Extracting SAE features from PII examples...")
# (Feature extraction logic here - use SAE encoder on text)

# Train classifiers
print("Training Random Forest classifier...")
rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=10)
# rf_classifier.fit(X_train, y_train)

print("Training Logistic Regression classifier...")
lr_classifier = LogisticRegression(max_iter=1000)
# lr_classifier.fit(X_train, y_train)

print("Training XGBoost classifier...")
xgb_classifier = xgb.XGBClassifier(n_estimators=100)
# xgb_classifier.fit(X_train, y_train)

# Save classifiers
with open("models/pii_classifiers.pkl", "wb") as f:
    pickle.dump({
        "rf": rf_classifier,
        "lr": lr_classifier,
        "xgb": xgb_classifier
    }, f)

print("âœ… PII classifiers trained and saved")
PYTHON

echo ""
echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘  SAE PII training complete! âœ…"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
SCRIPT

chmod +x train_sae_pii.sh
```

---

## ðŸŽ¯ Recommended Execution Plan

### Timeline (Single A100 40GB - Sequential)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Day 1 (Lambda)                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 00:00 - Launch A100 instance                               â”‚
â”‚ 00:30 - Setup & transfer data                              â”‚
â”‚ 01:00 - Start QA agent fine-tuning                         â”‚
â”‚ 05:00 - QA complete, start Support agent                   â”‚
â”‚ 09:00 - Support complete, start Legal agent                â”‚
â”‚ 13:00 - Legal complete, start Analyst agent                â”‚
â”‚ 17:00 - Analyst complete, start Content agent              â”‚
â”‚ 21:00 - Content complete âœ…                                â”‚
â”‚       â†’ 5 agents fine-tuned (20 hours, $22)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Day 2 (Lambda - Same Instance)                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 21:00 - Start SAE encoder training                         â”‚
â”‚ 09:00 - SAE encoder complete (12 hours)                    â”‚
â”‚ 09:30 - Generate PII dataset (30 min)                      â”‚
â”‚ 11:00 - Train PII classifiers (1.5 hours)                  â”‚
â”‚ 12:30 - SAE PII complete âœ…                                â”‚
â”‚       â†’ SAE training (15.5 hours, $17)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total: ~35.5 hours, $39 (single A100)
```

### Timeline (Dual A100 - Parallel)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GPU 1: Cross-Agent Fine-Tuning                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 00:00 - 04:00 â†’ QA agent                                   â”‚
â”‚ 04:00 - 08:00 â†’ Support agent                              â”‚
â”‚ 08:00 - 12:00 â†’ Legal agent                                â”‚
â”‚ 12:00 - 16:00 â†’ Analyst agent                              â”‚
â”‚ 16:00 - 20:00 â†’ Content agent                              â”‚
â”‚ 20:00 - Done âœ…                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GPU 2: SAE PII Training (Parallel)                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 00:00 - 12:00 â†’ SAE encoder training                       â”‚
â”‚ 12:00 - 12:30 â†’ Generate PII dataset                       â”‚
â”‚ 12:30 - 14:00 â†’ Train PII classifiers                      â”‚
â”‚ 14:00 - Done âœ…                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total: 20 hours (parallel), $44 (dual A100 @ $2.20/hour)
Saves: 15 hours vs sequential!
```

---

## ðŸ“¦ Data Transfer Package

**Create complete transfer package:**

```bash
cd /home/genesis/genesis-rebuild

# Create comprehensive package
cat > prepare_lambda_package.sh << 'SCRIPT'
#!/bin/bash

echo "Preparing Lambda training package..."

# Create package directory
mkdir -p lambda_package/data/training
mkdir -p lambda_package/scripts
mkdir -p lambda_package/infrastructure

# Copy training data
cp data/training/*.jsonl lambda_package/data/training/
echo "âœ… Copied 99,990 training examples"

# Copy fine-tuning infrastructure
cp -r infrastructure/finetune lambda_package/infrastructure/
cp scripts/finetune*.py lambda_package/scripts/ 2>/dev/null || true
echo "âœ… Copied fine-tuning scripts"

# Copy SAE scripts (if they exist)
cp scripts/train_sae*.py lambda_package/scripts/ 2>/dev/null || true
cp infrastructure/sae_pii_detector.py lambda_package/infrastructure/ 2>/dev/null || true
echo "âœ… Copied SAE PII scripts"

# Create README
cat > lambda_package/README.md << 'README'
# Lambda Labs Training Package

## Contents
- data/training/*.jsonl - 99,990 cross-agent training examples
- infrastructure/finetune/ - Unsloth pipeline
- scripts/ - Training scripts

## Quick Start
1. Upload to Lambda: scp -r lambda_package ubuntu@LAMBDA_IP:/home/ubuntu/
2. SSH: ssh ubuntu@LAMBDA_IP
3. cd /home/ubuntu/lambda_package
4. Run: ./train_all_agents.sh
README

# Compress
tar -czf lambda_training_package.tar.gz lambda_package/
size=$(du -h lambda_training_package.tar.gz | cut -f1)

echo ""
echo "âœ… Package created: lambda_training_package.tar.gz ($size)"
echo ""
echo "Transfer to Lambda:"
echo "  scp lambda_training_package.tar.gz ubuntu@LAMBDA_IP:/home/ubuntu/"
SCRIPT

chmod +x prepare_lambda_package.sh
./prepare_lambda_package.sh
```

---

## ðŸŽ¯ Optimized Schedule

### Parallel Training (2Ã— A100 - Fastest)

**GPU 1: Cross-Agent (20 hours)**
```bash
# Run all 5 agent fine-tuning jobs
./train_all_agents.sh
```

**GPU 2: SAE PII (14 hours)**
```bash
# Run SAE training
./train_sae_pii.sh
```

**Benefits:**
- Both complete in 20 hours (vs 35 hours sequential)
- Save 15 hours
- Only $4 more ($44 vs $39)

### Sequential Training (1Ã— A100 - Budget)

**Day 1-2: Cross-Agent (20 hours, $22)**
```bash
./train_all_agents.sh
```

**Day 2-3: SAE PII (14 hours, $15)**
```bash
./train_sae_pii.sh
```

**Benefits:**
- Lower upfront cost
- Can stop and resume if needed
- Total: $37

---

## ðŸ“Š Cost Comparison

| Approach | GPUs | Time | Cost |
|----------|------|------|------|
| Sequential (1Ã— A100) | 1 | 35 hours | $39 |
| Parallel (2Ã— A100) | 2 | 20 hours | $44 |
| Budget (1Ã— RTX 4090) | 1 | 45 hours | $18 |

**Recommended:** Parallel (2Ã— A100) - saves 15 hours for just $5 more

---

## âœ… Final Deliverables

**After Lambda training completes:**

```
models/
â”œâ”€â”€ qa_agent_finetuned/           (Fine-tuned Llama-3.1-8B + LoRA adapters)
â”œâ”€â”€ support_agent_finetuned/
â”œâ”€â”€ legal_agent_finetuned/
â”œâ”€â”€ analyst_agent_finetuned/
â”œâ”€â”€ content_agent_finetuned/
â”œâ”€â”€ sae_layer12_8x.pt             (SAE encoder: 32,768 latents)
â””â”€â”€ pii_classifiers.pkl            (RF + LR + XGBoost classifiers)
```

**Transfer back to VPS:**
```bash
# On Lambda
tar -czf trained_models.tar.gz models/

# Transfer back
scp trained_models.tar.gz genesis@5.161.211.16:/home/genesis/genesis-rebuild/
```

**Deploy:**
```bash
# On VPS
cd /home/genesis/genesis-rebuild
tar -xzf trained_models.tar.gz

# Test fine-tuned QA agent
python3 -c "
from infrastructure.model_registry import ModelRegistry
registry = ModelRegistry()
response = registry.chat('qa_agent', [{'role': 'user', 'content': 'Debug auth error'}])
print(response)
"
```

---

## ðŸ’¡ Pro Tips

### 1. Use tmux/screen on Lambda
```bash
# So you can disconnect without stopping training
tmux new -s training
./train_all_agents.sh
# Ctrl+B, then D to detach
# tmux attach -t training  # to reconnect
```

### 2. Monitor Progress Remotely
```bash
# From your VPS
ssh ubuntu@LAMBDA_IP "tail -f /home/ubuntu/cross_agent_training.log"
```

### 3. Checkpoint Saving
Both scripts auto-save checkpoints every 500 steps, so you can resume if interrupted.

### 4. Validate Before Full Run
Test with 1 agent first (4 hours, $4.40) to ensure everything works.

---

## ðŸŽ¯ Summary

**You can absolutely do both together!**

**Best approach:**
1. **Parallel (2Ã— A100):** $44, 20 hours - FASTEST
2. **Sequential (1Ã— A100):** $39, 35 hours - BUDGET

**Total project cost:**
- Data generation: $6.67 âœ… DONE
- Lambda training: $39-44
- **Grand total: $46-51** (vs $540 traditional = 91% savings!)

**Expected results:**
- âœ… 5 fine-tuned agents (30-40% better)
- âœ… SAE PII detector (96% F1 score, <100ms)
- âœ… $500 saved vs traditional approach

---

**Ready to proceed?** Just need to launch Lambda instance and transfer the data! ðŸš€

