[pytest]
# Basic configuration
asyncio_mode = auto
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*

# Test markers for categorization
markers =
    # Priority markers
    critical: Critical tests that must pass before deployment
    smoke: Quick smoke tests for basic functionality
    slow: Tests that take more than 10 seconds

    # Component markers
    infrastructure: Infrastructure layer tests
    agents: Agent-specific tests
    orchestration: Orchestration system tests (HTDAG, HALO, AOP)
    integration: Integration tests requiring multiple components

    # Feature markers
    performance: Performance and benchmark tests
    security: Security-related tests
    error_handling: Error handling and resilience tests
    observability: OTEL and monitoring tests

    # Layer markers
    layer1: Layer 1 - Genesis Meta-Agent orchestration
    layer2: Layer 2 - Darwin self-improvement
    layer3: Layer 3 - A2A protocol communication
    layer5: Layer 5 - Swarm optimization

    # Test type markers
    unit: Unit tests (isolated, fast)
    e2e: End-to-end tests
    concurrency: Concurrency and thread safety tests

    # Stability markers
    flaky: Tests that may occasionally fail due to timing/environment
    known_issue: Tests for known issues (skip in CI)

# Output and reporting
console_output_style = progress
addopts =
    # Verbose output for CI/CD
    -v
    # Show local variables in tracebacks
    --showlocals
    # Short traceback format
    --tb=short
    # Strict markers (fail on unknown markers)
    --strict-markers
    # Show summary of all test outcomes
    -ra
    # Warnings
    --disable-warnings
    # Performance
    --maxfail=50

# Retry configuration for flaky tests
# Performance tests may be sensitive to system contention in CI/CD environments
# Tests marked with @pytest.mark.flaky will use these retries
#
# IMPORTANT: Global reruns=0 to avoid retrying all tests, only those explicitly marked
# For performance tests specifically:
#   - Use @pytest.mark.flaky(reruns=3, reruns_delay=2) for pytest-rerunfailures
#   - OR use @retry_with_exponential_backoff() from conftest.py for advanced retry logic
#
# Why retry for performance tests:
#   - Performance tests measure wall-clock time, affected by OS scheduling and system load
#   - In full test suites (418+ tests), CPU/memory contention causes intermittent failures
#   - Tests pass consistently in isolation, proving code correctness
#   - Retry logic ensures reliability without relaxing performance thresholds
#
# Retry strategies comparison:
#   - pytest-rerunfailures: Fixed delay (1s, 1s, 1s) - good for most cases
#   - exponential backoff: Growing delay (1s, 2s, 4s) - better for contention clearing
#
reruns = 0
reruns_delay = 1.0

# Coverage configuration
[coverage:run]
source = .
omit =
    */tests/*
    */venv/*
    */site-packages/*
    */__pycache__/*
    */conftest.py
    */test_*.py
    */SE-Agent/*
branch = True
parallel = True

[coverage:report]
precision = 2
show_missing = True
skip_covered = False
exclude_lines =
    # Standard exclusions
    pragma: no cover
    def __repr__
    raise AssertionError
    raise NotImplementedError
    if __name__ == .__main__.:
    if TYPE_CHECKING:
    @abstractmethod
    @abc.abstractmethod
    # Defensive programming
    pass
    \.\.\.
    # Debug code
    if DEBUG:
    if debug:

[coverage:html]
directory = htmlcov

# Test output formatting
junit_suite_name = genesis-rebuild

# Timeout configuration
timeout = 300
timeout_method = thread

# Asyncio configuration
asyncio_default_fixture_loop_scope = function

# Log configuration
log_cli = false
log_cli_level = WARNING
log_cli_format = %(asctime)s [%(levelname)8s] %(message)s
log_cli_date_format = %Y-%m-%d %H:%M:%S

log_file = test-logs/pytest.log
log_file_level = DEBUG
log_file_format = %(asctime)s [%(levelname)8s] [%(filename)s:%(lineno)d] %(message)s
log_file_date_format = %Y-%m-%d %H:%M:%S

# Filterwarnings - silence known safe warnings
filterwarnings =
    # Ignore deprecation warnings from dependencies
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning
    # Error on warnings from our code
    error:::genesis.*

# Minimum Python version
minversion = 7.0

# Test collection
norecursedirs =
    .git
    .github
    .pytest_cache
    __pycache__
    venv
    htmlcov
    .coverage
    SE-Agent

# xdist configuration (parallel test execution)
# Use -n auto in CI to run tests in parallel
# Example: pytest -n auto

# Benchmark configuration (for pytest-benchmark)
[pytest-benchmark]
min_rounds = 5
max_time = 2.0
min_time = 0.000005
timer = time.perf_counter
disable_gc = true
warmup = true
