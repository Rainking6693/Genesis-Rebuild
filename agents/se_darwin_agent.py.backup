"""
SE-Darwin Agent - Multi-Trajectory Evolution for Self-Improving Agents
Layer 2 enhancement: Combines Darwin evolution with SE-Agent multi-trajectory optimization

Based on:
- SE-Agent (arXiv 2508.02085): Multi-trajectory evolution with revision/recombination/refinement
- Darwin Gödel Machine (arXiv 2505.22954): Self-improving code evolution
- GitHub: github.com/JARVIS-Xs/SE-Agent

BREAKTHROUGH: Multi-trajectory parallel search space exploration
- Generates multiple solution trajectories in parallel
- Applies intelligent operators (revision, recombination, refinement)
- Empirically validates each trajectory via benchmarks
- Archives successful patterns for cross-trajectory learning
- Proven: Better diversity → higher peak performance

Key Features:
- Parallel trajectory generation (3-5 trajectories per iteration)
- Operator-based evolution (revision for failures, recombination for successes)
- Benchmark-based validation (objective empirical scoring)
- TrajectoryPool integration (cross-iteration learning)
- OTEL observability (distributed tracing + metrics)

Architecture:
1. Initial trajectory generation (baseline approaches)
2. Parallel execution with timeout handling
3. Operator application based on results:
   - Failed → RevisionOperator (alternative strategy)
   - Successful → RecombinationOperator (crossover)
   - Promising → RefinementOperator (optimization)
4. Empirical validation via benchmarks
5. Archive best trajectories to pool
6. Iterate until convergence or max iterations

Integration Points:
- HTDAG orchestration (receives decomposed tasks)
- HALO router (executes trajectory-specific subtasks)
- TrajectoryPool (stores/retrieves evolution history)
- BenchmarkRunner (validates trajectory quality)
"""

import asyncio
import ast
import hashlib
import json
import logging
import time
import uuid
from dataclasses import dataclass, field
from datetime import datetime, timezone
from enum import Enum
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple, Set

# Genesis infrastructure
from infrastructure import get_logger
from infrastructure.trajectory_pool import (
    Trajectory,
    TrajectoryPool,
    TrajectoryStatus,
    OperatorType,
    get_trajectory_pool
)
from infrastructure.se_operators import (
    RevisionOperator,
    RecombinationOperator,
    RefinementOperator,
    OperatorResult,
    get_revision_operator,
    get_recombination_operator,
    get_refinement_operator
)
from infrastructure.benchmark_runner import BenchmarkRunner, BenchmarkResult, BenchmarkType
from infrastructure.security_utils import sanitize_agent_name, redact_credentials

# OTEL observability
try:
    from opentelemetry import trace, metrics
    from opentelemetry.trace import Status, StatusCode
    tracer = trace.get_tracer(__name__)
    meter = metrics.get_meter(__name__)

    # Metrics
    trajectory_counter = meter.create_counter(
        "se_darwin.trajectories.generated",
        description="Number of trajectories generated"
    )
    success_counter = meter.create_counter(
        "se_darwin.trajectories.successful",
        description="Number of successful trajectories"
    )
    operator_counter = meter.create_counter(
        "se_darwin.operators.applied",
        description="Number of operators applied"
    )
    execution_time_histogram = meter.create_histogram(
        "se_darwin.execution.duration",
        description="Trajectory execution time in seconds"
    )
except ImportError:
    # Graceful degradation if OTEL not available
    tracer = None
    trajectory_counter = None
    success_counter = None
    operator_counter = None
    execution_time_histogram = None


logger = get_logger("se_darwin_agent")


@dataclass
class ProductionOutcome:
    """
    Production execution outcome for learning.

    Captures real-world task execution results to enable
    SE-Darwin to learn from production experience.
    """
    outcome_id: str
    task_description: str
    agent_name: str
    execution_path: List[str]  # Sequence of steps/tools used
    result: Dict[str, Any]  # Final execution result
    success: bool
    error_message: Optional[str] = None
    execution_time: float = 0.0
    timestamp: str = field(default_factory=lambda: datetime.now(timezone.utc).isoformat())
    context: Dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for storage"""
        return {
            'outcome_id': self.outcome_id,
            'task_description': self.task_description,
            'agent_name': self.agent_name,
            'execution_path': self.execution_path,
            'result': self.result,
            'success': self.success,
            'error_message': self.error_message,
            'execution_time': self.execution_time,
            'timestamp': self.timestamp,
            'context': self.context
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "ProductionOutcome":
        """Create from dictionary"""
        return cls(**data)


@dataclass
class ExtractedPlan:
    """
    Plan extracted from production outcome.

    Reverse-engineers the strategy/plan that led to a
    successful or failed outcome for SE-Darwin training.
    """
    plan_id: str
    outcome_id: str  # Link to source outcome
    strategy_description: str  # What was the approach?
    reasoning_pattern: str  # How was it solved?
    tools_sequence: List[str]  # Tools used in order
    key_decisions: List[str]  # Critical decision points
    success_factors: List[str]  # What made it succeed?
    failure_factors: List[str]  # What made it fail?
    confidence: float = 0.0  # Plan extraction confidence (0-1)
    timestamp: str = field(default_factory=lambda: datetime.now(timezone.utc).isoformat())

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for storage"""
        return {
            'plan_id': self.plan_id,
            'outcome_id': self.outcome_id,
            'strategy_description': self.strategy_description,
            'reasoning_pattern': self.reasoning_pattern,
            'tools_sequence': self.tools_sequence,
            'key_decisions': self.key_decisions,
            'success_factors': self.success_factors,
            'failure_factors': self.failure_factors,
            'confidence': self.confidence,
            'timestamp': self.timestamp
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "ExtractedPlan":
        """Create from dictionary"""
        return cls(**data)


class OutcomeTrajectoryLogger:
    """
    Logs production outcomes and extracts plans for SE-Darwin learning.

    Phase 6 Enhancement: Enables continuous self-improvement by capturing
    real-world execution patterns and reverse-engineering successful strategies.

    Workflow:
    1. Production agents call log_outcome() after task execution
    2. OutcomeTrajectoryLogger stores outcome to Memory Store
    3. extract_plan() reverse-engineers the strategy from execution path
    4. Extracted plans stored as training examples for SE-Darwin
    5. SE-Darwin uses these plans to generate better trajectories

    Integration:
    - Memory Store: Persistent storage (namespace: "outcomes", "plans")
    - SE-Darwin: Consumes extracted plans for trajectory generation
    - Production Agents: Log outcomes via simple API

    Usage:
        logger = OutcomeTrajectoryLogger(memory_store)

        # After production task execution
        outcome = await logger.log_outcome(
            task="Build FastAPI endpoint",
            result={"status": "success", "endpoint": "/api/users"},
            success=True,
            execution_path=["analyze_spec", "generate_code", "test_endpoint"],
            agent_name="builder"
        )

        # Extract plan for learning
        plan = await logger.extract_plan(outcome)

        # Store as SE-Darwin training example
        await logger.store_training_example(outcome, plan)
    """

    def __init__(
        self,
        memory_store=None,  # GenesisMemoryStore instance
        llm_client=None,  # LLM for plan extraction (optional)
        enable_auto_extraction: bool = True
    ):
        """
        Initialize outcome trajectory logger.

        Args:
            memory_store: GenesisMemoryStore for persistent storage
            llm_client: Optional LLM client for intelligent plan extraction
            enable_auto_extraction: If True, automatically extract plans from outcomes
        """
        self.memory_store = memory_store
        self.llm_client = llm_client
        self.enable_auto_extraction = enable_auto_extraction

        # Statistics
        self.outcomes_logged = 0
        self.plans_extracted = 0
        self.training_examples_stored = 0

        logger.info(
            "OutcomeTrajectoryLogger initialized",
            extra={
                'has_memory_store': memory_store is not None,
                'has_llm': llm_client is not None,
                'auto_extraction': enable_auto_extraction
            }
        )

    async def log_outcome(
        self,
        task: str,
        result: Dict[str, Any],
        success: bool,
        execution_path: List[str],
        agent_name: str,
        error_message: Optional[str] = None,
        execution_time: float = 0.0,
        context: Optional[Dict[str, Any]] = None
    ) -> ProductionOutcome:
        """
        Log production outcome for learning.

        Args:
            task: Task description
            result: Execution result dictionary
            success: Whether task succeeded
            execution_path: Sequence of steps/tools used
            agent_name: Name of executing agent
            error_message: Optional error if failed
            execution_time: Execution time in seconds
            context: Optional additional context

        Returns:
            ProductionOutcome object
        """
        outcome_id = f"outcome_{agent_name}_{uuid.uuid4().hex[:8]}"

        outcome = ProductionOutcome(
            outcome_id=outcome_id,
            task_description=task,
            agent_name=agent_name,
            execution_path=execution_path,
            result=result,
            success=success,
            error_message=error_message,
            execution_time=execution_time,
            context=context or {}
        )

        # Store to memory if available
        if self.memory_store:
            try:
                await self.memory_store.save_memory(
                    namespace=("outcomes", agent_name),
                    key=outcome_id,
                    value=outcome.to_dict(),
                    tags=["production", "outcome", "success" if success else "failure"],
                    index_for_search=True
                )
                logger.info(
                    f"Outcome logged: {outcome_id} (success={success})",
                    extra={
                        'outcome_id': outcome_id,
                        'agent': agent_name,
                        'success': success,
                        'execution_time': execution_time
                    }
                )
            except Exception as e:
                logger.error(f"Failed to store outcome {outcome_id}: {e}")

        self.outcomes_logged += 1

        # Auto-extract plan if enabled
        if self.enable_auto_extraction and success:
            try:
                plan = await self.extract_plan(outcome)
                await self.store_training_example(outcome, plan)
            except Exception as e:
                logger.warning(f"Auto plan extraction failed for {outcome_id}: {e}")

        return outcome

    async def extract_plan(
        self,
        outcome: ProductionOutcome,
        use_llm: bool = False
    ) -> ExtractedPlan:
        """
        Extract plan from production outcome.

        Reverse-engineers the strategy that led to success/failure.

        Strategy:
        - Rule-based (default): Analyze execution path patterns
        - LLM-based (if llm_client provided): Use AI to infer strategy

        Args:
            outcome: ProductionOutcome to analyze
            use_llm: If True and llm_client available, use LLM extraction

        Returns:
            ExtractedPlan with strategy details
        """
        plan_id = f"plan_{outcome.outcome_id}"

        if use_llm and self.llm_client:
            # Future: LLM-based plan extraction
            logger.debug(f"LLM plan extraction not yet implemented for {outcome.outcome_id}")
            use_llm = False

        # Rule-based plan extraction (deterministic)
        strategy_description = self._infer_strategy(outcome)
        reasoning_pattern = self._infer_reasoning_pattern(outcome)
        key_decisions = self._extract_key_decisions(outcome)
        success_factors = self._analyze_success_factors(outcome) if outcome.success else []
        failure_factors = self._analyze_failure_factors(outcome) if not outcome.success else []

        plan = ExtractedPlan(
            plan_id=plan_id,
            outcome_id=outcome.outcome_id,
            strategy_description=strategy_description,
            reasoning_pattern=reasoning_pattern,
            tools_sequence=outcome.execution_path,
            key_decisions=key_decisions,
            success_factors=success_factors,
            failure_factors=failure_factors,
            confidence=0.8 if not use_llm else 0.95  # Rule-based: 0.8, LLM: 0.95
        )

        self.plans_extracted += 1

        logger.debug(
            f"Plan extracted from {outcome.outcome_id}",
            extra={
                'plan_id': plan_id,
                'strategy': strategy_description[:50],
                'confidence': plan.confidence
            }
        )

        return plan

    async def store_training_example(
        self,
        outcome: ProductionOutcome,
        plan: ExtractedPlan
    ) -> str:
        """
        Store outcome+plan pair as SE-Darwin training example.

        Args:
            outcome: Production outcome
            plan: Extracted plan

        Returns:
            Training example ID
        """
        if not self.memory_store:
            logger.warning("No memory store configured, training example not persisted")
            return ""

        training_id = f"training_{plan.plan_id}"

        training_example = {
            'training_id': training_id,
            'outcome': outcome.to_dict(),
            'plan': plan.to_dict(),
            'created_at': datetime.now(timezone.utc).isoformat()
        }

        try:
            await self.memory_store.save_memory(
                namespace=("training", outcome.agent_name),
                key=training_id,
                value=training_example,
                tags=["se_darwin", "training", "production_learning"],
                index_for_search=True
            )

            self.training_examples_stored += 1

            logger.info(
                f"Training example stored: {training_id}",
                extra={
                    'training_id': training_id,
                    'agent': outcome.agent_name,
                    'success': outcome.success,
                    'confidence': plan.confidence
                }
            )

            return training_id

        except Exception as e:
            logger.error(f"Failed to store training example {training_id}: {e}")
            return ""

    def _infer_strategy(self, outcome: ProductionOutcome) -> str:
        """Infer strategy description from execution path"""
        path_str = " → ".join(outcome.execution_path)
        return f"Sequential execution: {path_str}"

    def _infer_reasoning_pattern(self, outcome: ProductionOutcome) -> str:
        """Infer reasoning pattern from execution characteristics"""
        if len(outcome.execution_path) <= 2:
            return "direct_implementation"
        elif len(outcome.execution_path) <= 5:
            return "iterative_refinement"
        else:
            return "complex_multi_step"

    def _extract_key_decisions(self, outcome: ProductionOutcome) -> List[str]:
        """Extract key decision points from execution path"""
        # Simple heuristic: Each step is a decision
        return [f"Step {i+1}: {step}" for i, step in enumerate(outcome.execution_path)]

    def _analyze_success_factors(self, outcome: ProductionOutcome) -> List[str]:
        """Analyze factors contributing to success"""
        factors = []
        if outcome.execution_time < 5.0:
            factors.append("Fast execution (< 5s)")
        if len(outcome.execution_path) <= 5:
            factors.append("Efficient path (≤ 5 steps)")
        if outcome.result.get("status") == "success":
            factors.append("Explicit success status in result")
        return factors

    def _analyze_failure_factors(self, outcome: ProductionOutcome) -> List[str]:
        """Analyze factors contributing to failure"""
        factors = []
        if outcome.error_message:
            factors.append(f"Error: {outcome.error_message[:100]}")
        if outcome.execution_time > 30.0:
            factors.append("Slow execution (> 30s)")
        if len(outcome.execution_path) > 10:
            factors.append("Inefficient path (> 10 steps)")
        return factors

    def get_statistics(self) -> Dict[str, int]:
        """Get logger statistics"""
        return {
            'outcomes_logged': self.outcomes_logged,
            'plans_extracted': self.plans_extracted,
            'training_examples_stored': self.training_examples_stored
        }


class BenchmarkScenarioLoader:
    """
    Loads and manages benchmark scenarios from JSON files.

    Caches scenarios for performance and provides matching capabilities
    to find relevant benchmarks for a given problem description.
    """

    def __init__(self, benchmark_dir: Path = None):
        """
        Initialize benchmark scenario loader.

        Args:
            benchmark_dir: Path to directory containing benchmark JSON files
        """
        if benchmark_dir is None:
            # Default to benchmarks/test_cases relative to project root
            project_root = Path(__file__).parent.parent
            benchmark_dir = project_root / "benchmarks" / "test_cases"

        self.benchmark_dir = Path(benchmark_dir)
        self._scenarios_cache: Dict[str, List[Dict[str, Any]]] = {}
        self._load_all_scenarios()

    def _load_all_scenarios(self) -> None:
        """Load all benchmark scenarios from JSON files into cache"""
        if not self.benchmark_dir.exists():
            logger.warning(f"Benchmark directory not found: {self.benchmark_dir}")
            return

        for json_file in self.benchmark_dir.glob("*.json"):
            try:
                with open(json_file, 'r') as f:
                    scenarios = json.load(f)
                    agent_name = json_file.stem.replace("_scenarios", "")
                    self._scenarios_cache[agent_name] = scenarios
                    logger.debug(f"Loaded {len(scenarios)} scenarios for {agent_name}")
            except Exception as e:
                logger.error(f"Failed to load scenarios from {json_file}: {e}")

    def get_scenarios_for_agent(self, agent_name: str) -> List[Dict[str, Any]]:
        """
        Get all scenarios for a specific agent.

        Args:
            agent_name: Name of agent (e.g., 'builder', 'marketing')

        Returns:
            List of scenario dictionaries
        """
        return self._scenarios_cache.get(agent_name, [])

    def find_matching_scenario(
        self,
        agent_name: str,
        problem_description: str
    ) -> Optional[Dict[str, Any]]:
        """
        Find best matching scenario for a problem description.

        Uses simple keyword matching and description similarity.

        Args:
            agent_name: Name of agent
            problem_description: Problem description to match

        Returns:
            Best matching scenario dict or None
        """
        scenarios = self.get_scenarios_for_agent(agent_name)
        if not scenarios:
            return None

        problem_lower = problem_description.lower()
        problem_words = set(problem_lower.split())

        best_match = None
        best_score = 0.0

        for scenario in scenarios:
            scenario_desc = scenario.get("description", "").lower()
            scenario_words = set(scenario_desc.split())

            # Calculate simple word overlap score
            common_words = problem_words & scenario_words
            if len(problem_words) > 0:
                overlap_score = len(common_words) / len(problem_words)
            else:
                overlap_score = 0.0

            # Bonus for substring match
            if any(word in scenario_desc for word in problem_words if len(word) > 3):
                overlap_score += 0.3

            if overlap_score > best_score:
                best_score = overlap_score
                best_match = scenario

        return best_match if best_score > 0.2 else None

    def get_all_scenarios(self) -> List[Dict[str, Any]]:
        """Get all loaded scenarios across all agents"""
        all_scenarios = []
        for scenarios in self._scenarios_cache.values():
            all_scenarios.extend(scenarios)
        return all_scenarios


class CodeQualityValidator:
    """
    Deterministic code quality validation using AST analysis.

    Replaces random scoring with real metrics:
    - Syntax validation (AST parsing)
    - Import safety checks
    - Function signature validation
    - Docstring completeness
    - Type hint coverage

    P2-2 Fix: Non-deterministic scoring replaced with AST-based metrics.
    """

    @staticmethod
    def validate_code(
        code: str,
        expected_patterns: Optional[List[str]] = None,
        required_imports: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        Validate code quality using AST analysis.

        Args:
            code: Python code string to validate
            expected_patterns: Expected patterns/keywords in code
            required_imports: Required import names

        Returns:
            Dict with validation results:
            {
                'syntax_valid': bool,
                'import_score': float,
                'function_score': float,
                'docstring_score': float,
                'type_hint_score': float,
                'overall_score': float,
                'details': {...}
            }
        """
        result = {
            'syntax_valid': False,
            'import_score': 0.0,
            'function_score': 0.0,
            'docstring_score': 0.0,
            'type_hint_score': 0.0,
            'overall_score': 0.0,
            'details': {}
        }

        if not code or len(code.strip()) < 10:
            result['details']['error'] = 'Code too short or empty'
            return result

        # 1. Syntax validation (30% weight)
        try:
            tree = ast.parse(code)
            result['syntax_valid'] = True
            syntax_score = 1.0
        except SyntaxError as e:
            result['details']['syntax_error'] = str(e)
            syntax_score = 0.0
            # Cannot continue AST analysis without valid syntax
            result['overall_score'] = 0.0
            return result

        # 2. Import validation (20% weight)
        result['import_score'] = CodeQualityValidator._validate_imports(
            tree, required_imports or []
        )

        # 3. Function signature validation (20% weight)
        result['function_score'] = CodeQualityValidator._validate_functions(tree)

        # 4. Docstring completeness (15% weight)
        result['docstring_score'] = CodeQualityValidator._validate_docstrings(tree)

        # 5. Type hint coverage (15% weight)
        result['type_hint_score'] = CodeQualityValidator._validate_type_hints(tree)

        # Calculate overall weighted score
        result['overall_score'] = (
            syntax_score * 0.30 +
            result['import_score'] * 0.20 +
            result['function_score'] * 0.20 +
            result['docstring_score'] * 0.15 +
            result['type_hint_score'] * 0.15
        )

        # Store metrics in details
        result['details']['num_functions'] = len([
            n for n in ast.walk(tree) if isinstance(n, (ast.FunctionDef, ast.AsyncFunctionDef))
        ])
        result['details']['num_classes'] = len([
            n for n in ast.walk(tree) if isinstance(n, ast.ClassDef)
        ])
        result['details']['lines_of_code'] = len(code.split('\n'))

        return result

    @staticmethod
    def _validate_imports(tree: ast.AST, required_imports: List[str]) -> float:
        """Validate import statements (security + required imports)"""
        imports = []
        dangerous_imports = {'os', 'subprocess', 'eval', 'exec', 'compile', '__import__'}

        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                imports.extend([alias.name for alias in node.names])
            elif isinstance(node, ast.ImportFrom):
                if node.module:
                    imports.append(node.module)

        score = 1.0

        # Penalize dangerous imports
        found_dangerous = dangerous_imports & set(imports)
        if found_dangerous:
            score -= 0.3  # Security penalty

        # Check required imports
        if required_imports:
            found_required = sum(1 for req in required_imports if any(req in imp for imp in imports))
            required_ratio = found_required / len(required_imports) if required_imports else 1.0
            score = score * 0.5 + required_ratio * 0.5

        return max(0.0, min(1.0, score))

    @staticmethod
    def _validate_functions(tree: ast.AST) -> float:
        """Validate function definitions (proper structure)"""
        functions = [
            n for n in ast.walk(tree)
            if isinstance(n, (ast.FunctionDef, ast.AsyncFunctionDef))
        ]

        if not functions:
            return 0.5  # Neutral score if no functions

        # Check for basic function quality indicators
        has_args = sum(1 for f in functions if len(f.args.args) > 0)
        has_body = sum(1 for f in functions if len(f.body) > 0)

        score = (has_args / len(functions)) * 0.5 + (has_body / len(functions)) * 0.5

        return score

    @staticmethod
    def _validate_docstrings(tree: ast.AST) -> float:
        """Validate docstring presence and quality"""
        definitions = [
            n for n in ast.walk(tree)
            if isinstance(n, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef))
        ]

        if not definitions:
            return 1.0  # No definitions to document

        with_docstrings = sum(
            1 for node in definitions
            if ast.get_docstring(node) is not None
        )

        return with_docstrings / len(definitions)

    @staticmethod
    def _validate_type_hints(tree: ast.AST) -> float:
        """Validate type hint coverage"""
        functions = [
            n for n in ast.walk(tree)
            if isinstance(n, (ast.FunctionDef, ast.AsyncFunctionDef))
        ]

        if not functions:
            return 1.0  # No functions to type

        total_params = 0
        typed_params = 0
        return_typed = 0

        for func in functions:
            # Check parameter type hints
            for arg in func.args.args:
                total_params += 1
                if arg.annotation is not None:
                    typed_params += 1

            # Check return type hints
            if func.returns is not None:
                return_typed += 1

        param_score = typed_params / total_params if total_params > 0 else 1.0
        return_score = return_typed / len(functions) if functions else 1.0

        return (param_score * 0.6 + return_score * 0.4)


class EvolutionStatus(Enum):
    """Status of evolution iteration"""
    INITIALIZING = "initializing"
    GENERATING = "generating"
    EXECUTING = "executing"
    VALIDATING = "validating"
    ARCHIVING = "archiving"
    COMPLETED = "completed"
    FAILED = "failed"


@dataclass
class TrajectoryExecutionResult:
    """Result from executing a single trajectory"""
    trajectory: Trajectory
    benchmark_result: Optional[BenchmarkResult]
    execution_time: float
    success: bool
    error_message: Optional[str] = None


@dataclass
class EvolutionIteration:
    """Single iteration of evolution process"""
    iteration_id: str
    generation: int
    status: str  # EvolutionStatus
    trajectories_generated: int
    trajectories_successful: int
    best_score: float
    execution_time: float
    timestamp: str
    operator_stats: Dict[str, int] = field(default_factory=dict)


class SEDarwinAgent:
    """
    Multi-Trajectory Evolution Agent

    Implements SE-Agent's multi-trajectory optimization strategy:
    1. Generate multiple solution trajectories in parallel
    2. Apply evolution operators based on trajectory status
    3. Validate trajectories empirically via benchmarks
    4. Archive successful patterns to trajectory pool
    5. Iterate until convergence or max iterations

    Usage:
        agent = SEDarwinAgent(
            agent_name="builder",
            llm_client=openai_client,
            trajectories_per_iteration=3
        )
        result = await agent.evolve_solution(
            problem_description="Build FastAPI service with auth",
            max_iterations=3
        )
    """

    def __init__(
        self,
        agent_name: str,
        llm_client=None,
        trajectories_per_iteration: int = 3,
        max_iterations: int = 3,
        timeout_per_trajectory: int = 300,
        success_threshold: float = 0.7,
        benchmark_type: BenchmarkType = BenchmarkType.GENESIS_CUSTOM,
        memory_store=None  # NEW: Optional GenesisMemoryStore for production learning
    ):
        """
        Initialize SE-Darwin agent

        Args:
            agent_name: Name of agent being evolved
            llm_client: LLM client for operator generation (OpenAI/Anthropic)
            trajectories_per_iteration: Number of parallel trajectories to generate
            max_iterations: Maximum evolution iterations
            timeout_per_trajectory: Max seconds per trajectory execution
            success_threshold: Score threshold for success (0.7 = 70%)
            benchmark_type: Type of benchmark for validation
            memory_store: Optional GenesisMemoryStore for continuous learning
        """
        self.agent_name = sanitize_agent_name(agent_name)
        self.llm_client = llm_client
        self.trajectories_per_iteration = min(5, max(1, trajectories_per_iteration))  # Clamp 1-5
        self.max_iterations = max_iterations
        self.timeout_per_trajectory = timeout_per_trajectory
        self.success_threshold = success_threshold
        self.benchmark_type = benchmark_type
        self.memory_store = memory_store  # NEW: Store reference for production learning

        # Initialize components
        self.trajectory_pool = get_trajectory_pool(
            agent_name=agent_name,
            max_trajectories=50,
            load_existing=True
        )
        self.revision_operator = get_revision_operator(llm_client)
        self.recombination_operator = get_recombination_operator(llm_client)
        self.refinement_operator = get_refinement_operator(llm_client)
        self.benchmark_runner = BenchmarkRunner()

        # P2-1 Fix: Initialize benchmark scenario loader
        self.benchmark_loader = BenchmarkScenarioLoader()

        # Evolution state
        self.current_generation = 0
        self.best_score = 0.0
        self.best_trajectory_id: Optional[str] = None
        self.iterations: List[EvolutionIteration] = []

        logger.info(
            f"SEDarwinAgent initialized for {agent_name}",
            extra={
                'trajectories_per_iteration': trajectories_per_iteration,
                'max_iterations': max_iterations,
                'timeout': timeout_per_trajectory
            }
        )

    async def evolve_solution(
        self,
        problem_description: str,
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Main evolution loop - multi-trajectory optimization

        Args:
            problem_description: Problem to solve
            context: Additional context (code snippets, constraints, etc.)

        Returns:
            Dict containing best trajectory and evolution history
        """
        span_name = "se_darwin.evolve_solution"

        if tracer:
            with tracer.start_as_current_span(span_name) as span:
                span.set_attribute("agent.name", self.agent_name)
                span.set_attribute("max_iterations", self.max_iterations)
                return await self._evolve_solution_impl(problem_description, context)
        else:
            return await self._evolve_solution_impl(problem_description, context)

    async def _evolve_solution_impl(
        self,
        problem_description: str,
        context: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Implementation of evolution loop"""
        logger.info(f"Starting evolution for: {problem_description[:100]}...")
        start_time = time.time()

        context = context or {}

        # Evolution iterations
        for iteration in range(self.max_iterations):
            self.current_generation = iteration

            logger.info(f"\n{'='*60}")
            logger.info(f"ITERATION {iteration + 1}/{self.max_iterations}")
            logger.info(f"{'='*60}")

            iteration_start = time.time()

            # Generate trajectories for this iteration
            trajectories = await self._generate_trajectories(
                problem_description,
                context,
                iteration
            )

            logger.info(f"Generated {len(trajectories)} trajectories")

            # Execute trajectories in parallel
            execution_results = await self._execute_trajectories_parallel(
                trajectories,
                problem_description
            )

            # Analyze results and update best
            successful_count = sum(1 for r in execution_results if r.success)
            logger.info(f"Successful trajectories: {successful_count}/{len(execution_results)}")

            # Archive successful trajectories to pool
            await self._archive_trajectories(execution_results)

            # Update best score (track best overall, not just successful)
            for result in execution_results:
                if result.trajectory.success_score > self.best_score:
                    self.best_score = result.trajectory.success_score
                    self.best_trajectory_id = result.trajectory.trajectory_id
                    logger.info(f"New best score: {self.best_score:.3f} ({'successful' if result.success else 'not yet successful'})")

            # Record iteration
            iteration_time = time.time() - iteration_start
            self._record_iteration(
                iteration,
                len(trajectories),
                successful_count,
                iteration_time
            )

            # Check convergence
            if self._check_convergence(execution_results):
                logger.info("Convergence detected, stopping evolution")
                break

        total_time = time.time() - start_time

        # Get best trajectory
        best_trajectory = None
        if self.best_trajectory_id:
            best_trajectory = self.trajectory_pool.get_trajectory(self.best_trajectory_id)

        # Save trajectory pool
        self.trajectory_pool.save_to_disk()

        result = {
            'success': self.best_score > 0.0,  # Success if any score achieved
            'best_trajectory': best_trajectory,
            'best_score': self.best_score,
            'iterations': [
                {
                    'generation': it.generation,
                    'trajectories': it.trajectories_generated,
                    'successful': it.trajectories_successful,
                    'best_score': it.best_score,
                    'time': it.execution_time
                }
                for it in self.iterations
            ],
            'total_time': total_time,
            'pool_statistics': self.trajectory_pool.get_statistics()
        }

        logger.info(f"Evolution completed in {total_time:.2f}s, best score: {self.best_score:.3f}")

        return result

    async def _load_production_trajectories(
        self,
        limit: int = 10,
        success_only: bool = True
    ) -> List[Trajectory]:
        """
        Load production training examples from Memory Store

        Queries Memory Store for successful production outcomes and converts
        them into trajectories for SE-Darwin learning.

        Args:
            limit: Maximum number of trajectories to load
            success_only: If True, only load successful outcomes

        Returns:
            List of trajectories from production
        """
        trajectories = []

        if not hasattr(self, 'memory_store') or self.memory_store is None:
            logger.warning("Memory Store not configured, cannot load production trajectories")
            return trajectories

        try:
            # Query training examples namespace
            namespace = ("training", self.agent_name)
            keys = await self.memory_store.list_keys(namespace)

            # Limit to most recent training examples
            recent_keys = keys[:limit] if len(keys) > limit else keys

            logger.debug(
                f"Found {len(keys)} training examples, loading {len(recent_keys)}",
                extra={"namespace": namespace, "agent": self.agent_name}
            )

            # Load training examples
            for key in recent_keys:
                try:
                    training_data = await self.memory_store.get_memory(namespace, key)

                    if not training_data:
                        continue

                    # Extract outcome and plan from training example
                    outcome_data = training_data.get("outcome", {})
                    plan_data = training_data.get("plan", {})

                    # Reconstruct objects
                    outcome = ProductionOutcome.from_dict(outcome_data)
                    plan = ExtractedPlan.from_dict(plan_data)

                    # Filter by success if requested
                    if success_only and not outcome.success:
                        continue

                    # Convert to trajectory
                    trajectory = self._create_trajectory_from_production_plan(plan, outcome)
                    trajectories.append(trajectory)

                    logger.debug(
                        f"Loaded production trajectory: {trajectory.trajectory_id}",
                        extra={"plan_id": plan.plan_id, "confidence": plan.confidence}
                    )

                except Exception as e:
                    logger.warning(f"Failed to load training example {key}: {e}")
                    continue

            # Record OTEL metrics
            if tracer:
                try:
                    from opentelemetry import metrics
                    meter = metrics.get_meter(__name__)
                    production_counter = meter.create_counter(
                        "se_darwin.production_trajectories_loaded",
                        description="Number of production trajectories loaded"
                    )
                    production_counter.add(len(trajectories), {"agent": self.agent_name})

                    # Quality metric
                    if trajectories:
                        avg_confidence = sum(
                            float(t.metadata.get("confidence", 0.0)) for t in trajectories
                        ) / len(trajectories)

                        quality_histogram = meter.create_histogram(
                            "se_darwin.production_trajectory_quality",
                            description="Production trajectory quality (confidence)"
                        )
                        quality_histogram.record(avg_confidence, {"agent": self.agent_name})
                except Exception:
                    pass  # Silently fail OTEL if not available

            logger.info(
                f"Loaded {len(trajectories)} production trajectories from Memory Store",
                extra={"agent": self.agent_name, "limit": limit}
            )

        except Exception as e:
            logger.error(f"Failed to load production trajectories: {e}")

        return trajectories

    def _create_trajectory_from_production_plan(
        self,
        plan: ExtractedPlan,
        outcome: ProductionOutcome
    ) -> Trajectory:
        """
        Convert production plan + outcome into evolution trajectory

        Enables SE-Darwin to learn from real production executions by
        converting successful production outcomes into trainable trajectories.

        Args:
            plan: Extracted execution plan from production
            outcome: Production outcome that generated this plan

        Returns:
            Trajectory suitable for evolution training
        """
        # Generate trajectory ID
        trajectory_id = f"{self.agent_name}_production_{plan.plan_id}_{uuid.uuid4().hex[:8]}"

        # Extract code from production outcome (if available)
        code_changes = outcome.result.get("code", "") if isinstance(outcome.result, dict) else ""

        # Calculate validation results from success
        # Note: outcome_trajectory_logger uses duration_ms, not execution_time
        execution_time_seconds = outcome.duration_ms / 1000.0 if hasattr(outcome, 'duration_ms') else 0.0
        test_results = {
            "success": outcome.success,
            "execution_time": execution_time_seconds,
            "steps_completed": len(outcome.execution_path)
        }

        # Map plan fields to trajectory fields
        # Note: outcome_trajectory_logger ExtractedPlan has different field names than se_darwin_agent
        strategy = getattr(plan, 'strategy', getattr(plan, 'strategy_description', ''))
        reasoning = getattr(plan, 'reasoning_pattern', 'sequential')
        tools = getattr(plan, 'tools_sequence', getattr(plan, 'tools_required', []))

        # Create trajectory matching Trajectory dataclass schema
        trajectory = Trajectory(
            trajectory_id=trajectory_id,
            generation=self.current_generation,
            agent_name=self.agent_name,
            operator_applied=OperatorType.BASELINE.value,  # Production is baseline
            code_changes=code_changes,
            proposed_strategy=strategy,
            reasoning_pattern=reasoning,
            tools_used=tools,
            test_results=test_results,
            success_score=1.0 if outcome.success else 0.0,
            status=TrajectoryStatus.SUCCESS.value if outcome.success else TrajectoryStatus.FAILURE.value,
            execution_time_seconds=execution_time_seconds if hasattr(Trajectory, 'execution_time_seconds') else 0.0,
            failure_reasons=[] if outcome.success else [getattr(outcome, 'error_message', None) or "production_failure"]
        )

        # Add metadata via dictionary update (Trajectory may not have metadata field as class attribute)
        if not hasattr(trajectory, '_metadata'):
            trajectory._metadata = {}
        trajectory._metadata.update({
            "source": "production",
            "outcome_id": outcome.outcome_id,
            "plan_id": plan.plan_id,
            "confidence": plan.confidence
        })

        logger.info(
            f"Created trajectory from production: {trajectory_id}",
            extra={
                "outcome_id": outcome.outcome_id,
                "plan_id": plan.plan_id,
                "success": outcome.success,
                "confidence": plan.confidence
            }
        )

        return trajectory

    async def _generate_trajectories(
        self,
        problem_description: str,
        context: Dict[str, Any],
        generation: int
    ) -> List[Trajectory]:
        """
        Generate multiple trajectories for this iteration

        Strategy:
        - Iteration 0: Generate baseline trajectories (no operators)
        - Iteration 1+: Apply operators to previous trajectories
          - Failed → Revision (alternative strategy)
          - Successful pairs → Recombination (crossover)
          - Promising → Refinement (optimization)

        NEW: Also queries Memory Store for production training examples
        to enable continuous learning from real-world executions.

        Args:
            problem_description: Problem to solve
            context: Additional context
            generation: Current generation number

        Returns:
            List of trajectories to execute
        """
        trajectories = []

        # NEW: Query Memory Store for production training examples (all generations)
        if hasattr(self, 'memory_store') and self.memory_store is not None:
            try:
                production_trajectories = await self._load_production_trajectories(limit=10)

                # Add production trajectories to pool for learning
                for traj in production_trajectories:
                    trajectories.append(traj)

                    # Increment OTEL counter
                    if trajectory_counter:
                        trajectory_counter.add(1, {"operator": "production_learning"})

                # Record OTEL metric for production trajectories loaded
                if tracer:
                    from opentelemetry import trace
                    span = trace.get_current_span()
                    span.set_attribute("production_trajectories_loaded", len(production_trajectories))

                logger.info(
                    f"Loaded {len(production_trajectories)} production trajectories for learning",
                    extra={
                        "generation": generation,
                        "agent": self.agent_name
                    }
                )
            except Exception as e:
                logger.warning(f"Failed to load production trajectories: {e}")

        if generation == 0:
            # Initial generation: Create baseline trajectories
            for i in range(self.trajectories_per_iteration):
                trajectory = self._create_baseline_trajectory(
                    problem_description,
                    context,
                    generation,
                    i
                )
                trajectories.append(trajectory)

                if trajectory_counter:
                    trajectory_counter.add(1, {"operator": "baseline"})

        else:
            # Subsequent generations: Apply operators

            # 1. Revision: Generate from failed trajectories
            failed_trajectories = self.trajectory_pool.get_failed_trajectories()
            if failed_trajectories and len(trajectories) < self.trajectories_per_iteration:
                failed_traj = failed_trajectories[0]  # Take worst performer

                result = await self.revision_operator.revise(
                    failed_traj,
                    problem_description
                )

                if result.success:
                    trajectory = self._create_trajectory_from_operator(
                        result,
                        OperatorType.REVISION,
                        generation,
                        [failed_traj.trajectory_id]
                    )
                    trajectories.append(trajectory)

                    if operator_counter:
                        operator_counter.add(1, {"operator": "revision"})

            # 2. Recombination: Crossover of successful trajectories
            successful_pairs = self.trajectory_pool.get_diverse_successful_pairs(n=1)
            if successful_pairs and len(trajectories) < self.trajectories_per_iteration:
                traj_a, traj_b = successful_pairs[0]

                result = await self.recombination_operator.recombine(
                    traj_a,
                    traj_b,
                    problem_description
                )

                if result.success:
                    trajectory = self._create_trajectory_from_operator(
                        result,
                        OperatorType.RECOMBINATION,
                        generation,
                        [traj_a.trajectory_id, traj_b.trajectory_id]
                    )
                    trajectories.append(trajectory)

                    if operator_counter:
                        operator_counter.add(1, {"operator": "recombination"})

            # 3. Refinement: Optimize promising trajectories
            pool_insights = self.trajectory_pool.get_pool_insights(max_insights=10)
            successful = self.trajectory_pool.get_successful_trajectories()

            if successful and pool_insights and len(trajectories) < self.trajectories_per_iteration:
                promising_traj = successful[0]  # Best performer

                result = await self.refinement_operator.refine(
                    promising_traj,
                    pool_insights,
                    problem_description
                )

                if result.success:
                    trajectory = self._create_trajectory_from_operator(
                        result,
                        OperatorType.REFINEMENT,
                        generation,
                        [promising_traj.trajectory_id]
                    )
                    trajectories.append(trajectory)

                    if operator_counter:
                        operator_counter.add(1, {"operator": "refinement"})

            # 4. Fill remaining slots with baseline trajectories
            while len(trajectories) < self.trajectories_per_iteration:
                trajectory = self._create_baseline_trajectory(
                    problem_description,
                    context,
                    generation,
                    len(trajectories)
                )
                trajectories.append(trajectory)

                if trajectory_counter:
                    trajectory_counter.add(1, {"operator": "baseline"})

        return trajectories

    def _create_baseline_trajectory(
        self,
        problem_description: str,
        context: Dict[str, Any],
        generation: int,
        index: int
    ) -> Trajectory:
        """Create baseline trajectory without operators"""
        trajectory_id = f"{self.agent_name}_g{generation}_baseline_{index}_{uuid.uuid4().hex[:8]}"

        return Trajectory(
            trajectory_id=trajectory_id,
            generation=generation,
            agent_name=self.agent_name,
            operator_applied=OperatorType.BASELINE.value,
            proposed_strategy=f"Baseline approach {index} for: {problem_description[:50]}",
            reasoning_pattern="direct_implementation",
            status=TrajectoryStatus.PENDING.value
        )

    def _create_trajectory_from_operator(
        self,
        operator_result: OperatorResult,
        operator_type: OperatorType,
        generation: int,
        parent_ids: List[str]
    ) -> Trajectory:
        """Create trajectory from operator result"""
        trajectory_id = f"{self.agent_name}_g{generation}_{operator_type.value}_{uuid.uuid4().hex[:8]}"

        return Trajectory(
            trajectory_id=trajectory_id,
            generation=generation,
            agent_name=self.agent_name,
            parent_trajectories=parent_ids,
            operator_applied=operator_type.value,
            code_changes=operator_result.generated_code or "",
            proposed_strategy=operator_result.strategy_description,
            reasoning_pattern=operator_result.reasoning,
            status=TrajectoryStatus.PENDING.value
        )

    async def _execute_trajectories_parallel(
        self,
        trajectories: List[Trajectory],
        problem_description: str
    ) -> List[TrajectoryExecutionResult]:
        """
        Execute multiple trajectories in parallel with timeout

        Args:
            trajectories: Trajectories to execute
            problem_description: Original problem description

        Returns:
            List of execution results
        """
        logger.info(f"Executing {len(trajectories)} trajectories in parallel")

        # Create execution tasks
        tasks = [
            self._execute_single_trajectory(traj, problem_description)
            for traj in trajectories
        ]

        # Execute with timeout
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # Handle exceptions
        execution_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"Trajectory {trajectories[i].trajectory_id} failed: {result}")
                execution_results.append(
                    TrajectoryExecutionResult(
                        trajectory=trajectories[i],
                        benchmark_result=None,
                        execution_time=0.0,
                        success=False,
                        error_message=str(result)
                    )
                )
            else:
                execution_results.append(result)

        return execution_results

    async def _execute_single_trajectory(
        self,
        trajectory: Trajectory,
        problem_description: str
    ) -> TrajectoryExecutionResult:
        """
        Execute single trajectory with validation

        Args:
            trajectory: Trajectory to execute
            problem_description: Original problem description

        Returns:
            Execution result with benchmark scores
        """
        start_time = time.time()

        logger.info(f"Executing trajectory: {trajectory.trajectory_id}")

        # Update status
        trajectory.status = TrajectoryStatus.EXECUTING.value

        try:
            # Execute trajectory with timeout
            async with asyncio.timeout(self.timeout_per_trajectory):
                # Validate trajectory via benchmark
                # Note: In production, this would execute the code changes
                # For now, we use mock validation based on trajectory quality
                benchmark_result = await self._validate_trajectory(trajectory, problem_description)

                # Update trajectory with results
                trajectory.status = TrajectoryStatus.SUCCESS.value if benchmark_result.overall_score >= self.success_threshold else TrajectoryStatus.FAILURE.value
                trajectory.success_score = benchmark_result.overall_score
                trajectory.metrics = benchmark_result.metrics
                trajectory.test_results = {
                    'tasks_total': benchmark_result.tasks_total,
                    'tasks_passed': benchmark_result.tasks_passed,
                    'tasks_failed': benchmark_result.tasks_failed
                }

                execution_time = time.time() - start_time
                trajectory.execution_time_seconds = execution_time

                # Record metrics
                if execution_time_histogram:
                    execution_time_histogram.record(execution_time)

                if trajectory.is_successful(self.success_threshold) and success_counter:
                    success_counter.add(1)

                logger.info(
                    f"Trajectory {trajectory.trajectory_id} completed: score={trajectory.success_score:.3f}, time={execution_time:.2f}s"
                )

                return TrajectoryExecutionResult(
                    trajectory=trajectory,
                    benchmark_result=benchmark_result,
                    execution_time=execution_time,
                    success=trajectory.is_successful(self.success_threshold)
                )

        except asyncio.TimeoutError:
            logger.warning(f"Trajectory {trajectory.trajectory_id} timed out after {self.timeout_per_trajectory}s")
            trajectory.status = TrajectoryStatus.FAILURE.value
            trajectory.failure_reasons.append("execution_timeout")

            execution_time = time.time() - start_time

            return TrajectoryExecutionResult(
                trajectory=trajectory,
                benchmark_result=None,
                execution_time=execution_time,
                success=False,
                error_message="Execution timeout"
            )

        except Exception as e:
            logger.error(f"Trajectory {trajectory.trajectory_id} failed: {e}")
            trajectory.status = TrajectoryStatus.FAILURE.value
            trajectory.failure_reasons.append(f"execution_error: {str(e)}")

            execution_time = time.time() - start_time

            return TrajectoryExecutionResult(
                trajectory=trajectory,
                benchmark_result=None,
                execution_time=execution_time,
                success=False,
                error_message=str(e)
            )

    async def _validate_trajectory(
        self,
        trajectory: Trajectory,
        problem_description: str
    ) -> BenchmarkResult:
        """
        Validate trajectory via benchmark execution.

        P2-1 Fix: Uses real benchmark scenarios from JSON files.
        P2-2 Fix: Deterministic scoring using AST-based code quality validation.

        Process:
        1. Find matching benchmark scenario for problem description
        2. Extract code from trajectory
        3. Validate code using AST-based quality metrics
        4. Calculate score using weighted formula (no randomness)
        5. Return BenchmarkResult with deterministic scores

        Args:
            trajectory: Trajectory to validate
            problem_description: Original problem description

        Returns:
            BenchmarkResult with deterministic scores
        """
        # P2-1: Find matching benchmark scenario
        matching_scenario = self.benchmark_loader.find_matching_scenario(
            self.agent_name,
            problem_description
        )

        # Extract required patterns and imports from scenario
        required_imports = []
        expected_patterns = []
        quality_min = 0.7

        if matching_scenario:
            expected_outputs = matching_scenario.get("expected_outputs", {})
            required_imports = expected_outputs.get("required_imports", [])
            expected_patterns = expected_outputs.get("required_patterns", [])
            quality_min = expected_outputs.get("code_quality_min", 0.7)

        # P2-2: Deterministic code quality validation
        code = trajectory.code_changes or ""
        validation_result = CodeQualityValidator.validate_code(
            code,
            expected_patterns=expected_patterns,
            required_imports=required_imports
        )

        # Calculate final score using weighted formula
        base_score = validation_result['overall_score']

        # Bonus for operator diversity (deterministic)
        operator_bonus = 0.0
        if trajectory.operator_applied == OperatorType.RECOMBINATION.value:
            operator_bonus = 0.12
        elif trajectory.operator_applied == OperatorType.REFINEMENT.value:
            operator_bonus = 0.08
        elif trajectory.operator_applied == OperatorType.REVISION.value:
            operator_bonus = 0.04

        # Bonus for having substantial code changes (deterministic)
        code_bonus = 0.0
        if code and len(code.strip()) > 50:
            code_lines = len([line for line in code.split('\n') if line.strip()])
            code_bonus = min(0.10, code_lines / 200)  # Max 0.10 bonus

        # Bonus for having detailed strategy (deterministic)
        strategy_bonus = 0.0
        if trajectory.proposed_strategy and len(trajectory.proposed_strategy) > 20:
            strategy_words = len(trajectory.proposed_strategy.split())
            strategy_bonus = min(0.05, strategy_words / 200)  # Max 0.05 bonus

        # Final score calculation (weighted, deterministic)
        final_score = (
            base_score * 0.70 +      # Code quality: 70%
            operator_bonus +          # Operator type: up to 12%
            code_bonus +              # Code substance: up to 10%
            strategy_bonus            # Strategy detail: up to 5%
        )
        final_score = max(0.0, min(1.0, final_score))  # Clamp to [0, 1]

        # Calculate task pass/fail based on score
        tasks_total = 10
        tasks_passed = int(final_score * tasks_total)
        tasks_failed = tasks_total - tasks_passed

        # Deterministic execution time based on code complexity
        code_lines = validation_result['details'].get('lines_of_code', 0)
        num_functions = validation_result['details'].get('num_functions', 0)
        execution_time = 1.0 + (code_lines * 0.01) + (num_functions * 0.2)
        execution_time = min(5.0, execution_time)  # Cap at 5 seconds

        # Create benchmark result with deterministic metrics
        benchmark_result = BenchmarkResult(
            benchmark_id=f"bench_{trajectory.trajectory_id}",
            benchmark_type=self.benchmark_type.value,
            agent_name=self.agent_name,
            agent_version=trajectory.trajectory_id,
            status="completed",
            overall_score=final_score,
            metrics={
                'accuracy': final_score,
                'syntax_valid': 1.0 if validation_result['syntax_valid'] else 0.0,
                'import_score': validation_result['import_score'],
                'function_score': validation_result['function_score'],
                'docstring_score': validation_result['docstring_score'],
                'type_hint_score': validation_result['type_hint_score'],
                'operator_bonus': operator_bonus,
                'code_bonus': code_bonus,
                'strategy_bonus': strategy_bonus,
                'matched_scenario': matching_scenario['id'] if matching_scenario else None
            },
            tasks_total=tasks_total,
            tasks_passed=tasks_passed,
            tasks_failed=tasks_failed,
            execution_time=execution_time,
            timestamp=datetime.now(timezone.utc).isoformat()
        )

        logger.debug(
            f"Trajectory validation: score={final_score:.3f}, "
            f"syntax_valid={validation_result['syntax_valid']}, "
            f"scenario_match={'Yes' if matching_scenario else 'No'}"
        )

        return benchmark_result

    async def _archive_trajectories(
        self,
        execution_results: List[TrajectoryExecutionResult]
    ) -> None:
        """
        Archive trajectories to pool for cross-iteration learning

        Args:
            execution_results: Results to archive
        """
        for result in execution_results:
            self.trajectory_pool.add_trajectory(result.trajectory)

        logger.info(f"Archived {len(execution_results)} trajectories to pool")

    def _check_convergence(
        self,
        execution_results: List[TrajectoryExecutionResult]
    ) -> bool:
        """
        Check if evolution has converged

        Convergence criteria:
        - All trajectories successful (score >= threshold)
        - Best score hasn't improved in last 2 iterations
        - Best score exceeds 0.9 (90% quality)

        Args:
            execution_results: Results from current iteration

        Returns:
            True if converged
        """
        # Check if all successful
        all_successful = all(r.success for r in execution_results)

        # Check if best score plateaued
        recent_best_scores = [it.best_score for it in self.iterations[-2:]] if len(self.iterations) >= 2 else []
        score_plateaued = len(recent_best_scores) == 2 and abs(recent_best_scores[0] - recent_best_scores[1]) < 0.01

        # Check if excellent score achieved
        excellent_score = self.best_score >= 0.9

        converged = all_successful or score_plateaued or excellent_score

        if converged:
            logger.info(f"Convergence: all_successful={all_successful}, plateaued={score_plateaued}, excellent={excellent_score}")

        return converged

    def _record_iteration(
        self,
        generation: int,
        trajectories_generated: int,
        trajectories_successful: int,
        execution_time: float
    ) -> None:
        """Record iteration statistics"""
        iteration = EvolutionIteration(
            iteration_id=f"iter_{generation}_{uuid.uuid4().hex[:8]}",
            generation=generation,
            status=EvolutionStatus.COMPLETED.value,
            trajectories_generated=trajectories_generated,
            trajectories_successful=trajectories_successful,
            best_score=self.best_score,
            execution_time=execution_time,
            timestamp=datetime.now(timezone.utc).isoformat()
        )

        self.iterations.append(iteration)


# Factory functions
def get_se_darwin_agent(
    agent_name: str,
    llm_client=None,
    trajectories_per_iteration: int = 3,
    max_iterations: int = 3
) -> SEDarwinAgent:
    """
    Factory function to create SE-Darwin agent

    Args:
        agent_name: Name of agent
        llm_client: LLM client for operators
        trajectories_per_iteration: Number of parallel trajectories
        max_iterations: Max evolution iterations

    Returns:
        SEDarwinAgent instance
    """
    return SEDarwinAgent(
        agent_name=agent_name,
        llm_client=llm_client,
        trajectories_per_iteration=trajectories_per_iteration,
        max_iterations=max_iterations
    )


def get_outcome_logger(
    memory_store=None,
    llm_client=None,
    enable_auto_extraction: bool = True
) -> OutcomeTrajectoryLogger:
    """
    Factory function to create OutcomeTrajectoryLogger

    Args:
        memory_store: GenesisMemoryStore for persistent storage
        llm_client: Optional LLM client for intelligent plan extraction
        enable_auto_extraction: If True, automatically extract plans from outcomes

    Returns:
        OutcomeTrajectoryLogger instance
    """
    return OutcomeTrajectoryLogger(
        memory_store=memory_store,
        llm_client=llm_client,
        enable_auto_extraction=enable_auto_extraction
    )
